<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NUTS | Adam Haber</title>
    <link>https://adamhaber.github.io/tags/nuts/</link>
      <atom:link href="https://adamhaber.github.io/tags/nuts/index.xml" rel="self" type="application/rss+xml" />
    <description>NUTS</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 21 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://adamhaber.github.io/img/icon-192.png</url>
      <title>NUTS</title>
      <link>https://adamhaber.github.io/tags/nuts/</link>
    </image>
    
    <item>
      <title>Bayesian golf puttings, NUTS, and optimizing your sampling function with TensorFlow Probability</title>
      <link>https://adamhaber.github.io/post/nuts/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://adamhaber.github.io/post/nuts/</guid>
      <description>

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Port a great Bayesian modelling tutorial from Stan to TFP&lt;/li&gt;
&lt;li&gt;Discuss how to speed up our sampling function&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;trace_fn&lt;/code&gt; to produce Stan-like &lt;code&gt;generated quantities&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Explore the results using the ArviZ library.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;

&lt;p&gt;This is a TFP-port one of of the best Bayesian modelling tutorials I&amp;rsquo;ve seen online - the &lt;a href=&#34;https://mc-stan.org/users/documentation/case-studies/golf.html&#34; target=&#34;_blank&#34;&gt;Model building and expansion for golf putting&lt;/a&gt; Stan tutorial. It&amp;rsquo;s a beautiful example of modeling from first principles, and why the incorporation of domain knowledge into a statistical model - in this case, knowing a little bit about golf and some high-school physics - is so important. Since there&amp;rsquo;s no chance I&amp;rsquo;ll explain the subject nearly as well as Gelman, go read his tutorial and come back if you want to learn how to do this with TFP. :-)&lt;/p&gt;

&lt;p&gt;Other than the actual TFP code for the different models, this post will also shortly discuss the new NUTS kernel, various optimizations techniques to make sampling (much) faster, how to use &lt;code&gt;trace_fn&lt;/code&gt; to produce Stan-style generated quantities, and the ArviZ plotting library for inspecting sampler traces and statistics. I&amp;rsquo;m not an expert on any of these topics - I&amp;rsquo;ll share what I&amp;rsquo;ve learned while trying to implement these models, and provide links for further reading.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re more into basketball than golf, you might be interested in &lt;a href=&#34;https://jamesblandecon.github.io/NBAJumpShotsIndividual.html&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; case-study; it&amp;rsquo;s based on the golf tutorial, but analyzes NBA jump shots instead.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# the necessary imports
import tensorflow.compat.v2 as tf

import tensorflow_probability as tfp

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt 
import numpy as np
from time import time
from tensorflow_probability import distributions as tfd
from tensorflow_probability import bijectors as tfb
from functools import partial

import arviz as az  #we&#39;ll get to that
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.set_palette(&amp;quot;muted&amp;quot;)
np.random.seed(1324)
dtype=tf.float32
params = {
    &#39;legend.fontsize&#39;: &#39;x-large&#39;,
    &#39;figure.figsize&#39;: (9, 6),
    &#39;axes.labelsize&#39;: &#39;x-large&#39;,
    &#39;axes.titlesize&#39;:&#39;x-large&#39;,
    &#39;xtick.labelsize&#39;:&#39;x-large&#39;,
    &#39;ytick.labelsize&#39;:&#39;x-large&#39;
}
plt.rcParams.update(params)
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;arviz&#34;&gt;ArviZ&lt;/h1&gt;

&lt;p&gt;In the previous posts we&amp;rsquo;ve been using &lt;a href=&#34;https://seaborn.pydata.org&#34; target=&#34;_blank&#34;&gt;seaborn&lt;/a&gt; for plotting. Seaborn is an amazing plotting library with good defaults and well-designed API, but it was not built with MCMC simulations in mind. &lt;a href=&#34;https://arviz-devs.github.io/arviz/&#34; target=&#34;_blank&#34;&gt;ArviZ&lt;/a&gt; was. ArviZ allows for &amp;ldquo;Exploratory analysis of Bayesian models&amp;rdquo;, and interfaces with many bayesian modeling libraries such as PyStan, PyMC3, Pyro, emcee and TFP. ArviZ prints nice dataframes with summary statistics, supports model comparison using LOO and WAIC (similar to the &lt;a href=&#34;https://cran.r-project.org/web/packages/loo/loo.pdf&#34; target=&#34;_blank&#34;&gt;loo&lt;/a&gt; package in R), and a wide variety of diagnosis tools; it&amp;rsquo;s the most comprehensive Python package for this sort of analysis I&amp;rsquo;ve seen, and it&amp;rsquo;s maintained by top PyMC3 contributors.&lt;/p&gt;

&lt;p&gt;The only drawback for using ArviZ with TFP at the moment is that the &lt;code&gt;from_tfp&lt;/code&gt; function has troubles with multi-chain traces (which are pretty much the norm); I adjusted a code snippet from &lt;a href=&#34;https://colab.research.google.com/gist/junpenglao/51cd25c6372f8d2ab3490d4af8f97401/tfp_nuts_demo.ipynb&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; notebook and wrote a small helper function to handle this. The &lt;code&gt;tfp_trace_to_arviz&lt;/code&gt; function gets a &lt;code&gt;StatesAndTrace&lt;/code&gt; object, which is a container with two elements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a list of samples tensors (the actual output of the sampler).&lt;/li&gt;
&lt;li&gt;a list of trace statistics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip; and returns an ArviZ &lt;code&gt;InferenceData&lt;/code&gt; object. There is a wide range of statistics we can extract from our sampler, which are important for sampling diagnosis. This is the actual function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# using pymc3 naming conventions, with log_likelihood instead of lp so that ArviZ can compute loo and waic
sample_stats_name = [&#39;log_likelihood&#39;,&#39;tree_size&#39;,&#39;diverging&#39;,&#39;energy&#39;,&#39;mean_tree_accept&#39;]

def tfp_trace_to_arviz(
    tfp_trace,
    var_names=None, 
    sample_stats_name=sample_stats_name):
    
    samps, trace = tfp_trace
    if var_names is None:
        var_names = [&amp;quot;var &amp;quot; + str(x) for x in range(len(samps))]
        
    sample_stats = {k:v.numpy().T for k, v in zip(sample_stats_name, trace)}
    posterior = {name : tf.transpose(samp, [1, 0, 2]).numpy() for name, samp in zip(var_names, samps)}
    return az.from_dict(posterior=posterior, sample_stats=sample_stats)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only tricky thing here is the &lt;code&gt;tf.transpose&lt;/code&gt; operation - we&amp;rsquo;re making sure the chain dimension is the first axis, to be consistent with ArviZ conventions (see &lt;a href=&#34;https://github.com/arviz-devs/arviz/blob/24f8268844cf3cc5cf10d152e955c3122ec477c0/arviz/data/base.py#L103&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;az.data.numpy_to_data_array&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;h1 id=&#34;nuts&#34;&gt;NUTS&lt;/h1&gt;

&lt;p&gt;Previous posts have used the Hamiltonian Monte Carlo sampler to sample from our posterior distributions. When I tried to use HMC to sample from the models described below (specifically, from the 3rd and 4th models) I&amp;rsquo;ve had troubles reproducing the results from the Stan tutorial. I&amp;rsquo;ve posted a question on the TFP google group (super responsive and helpful), and Junpeng Lao (PyMC3 core developer, now working on TFP) recommended I&amp;rsquo;ll try the new NUTS kernel, instead. He said NUTS requires significantly less hand-tuning for complex models, compared to HMC; he was right.&lt;/p&gt;

&lt;p&gt;Since the posts so far were mostly code-oriented, we haven&amp;rsquo;t really gotten into the HMC algorithm, so we&amp;rsquo;re not going to get into why NUTS is an improvement; suffice to say it traverses the posterior distribution in a more efficient way, which is good for us (the simple people who don&amp;rsquo;t know how to hand-tune the number of leapfrog steps). We&amp;rsquo;ll see below another new addition to TFP that helps with another fine-tuning problem.&lt;/p&gt;

&lt;p&gt;For further reading, these are all excellent:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The original &lt;a href=&#34;http://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf&#34; target=&#34;_blank&#34;&gt;NUTS&lt;/a&gt; paper.&lt;/li&gt;
&lt;li&gt;Michael Betancourt&amp;rsquo;s &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=2ahUKEwisvufdvJflAhWl5OAKHctkBaYQFjABegQIAhAB&amp;amp;url=https%3A%2F%2Farxiv.org%2Fabs%2F1701.02434&amp;amp;usg=AOvVaw0KCc_YNSmoxjpHQOaZP2IN&#34; target=&#34;_blank&#34;&gt;A Conceptual Introduction to Hamiltonian Monte Carlo&lt;/a&gt; paper.&lt;/li&gt;
&lt;li&gt;For the more code-oriented reader - Colin Carroll&amp;rsquo;s excellent &lt;a href=&#34;https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/&#34; target=&#34;_blank&#34;&gt;series&lt;/a&gt; of posts on HMC, tuning, etc. Colin is one of the maintainers of both ArviZ and PyMC3.&lt;/li&gt;
&lt;li&gt;Sigrid Keydana&amp;rsquo;s &lt;a href=&#34;https://blogs.rstudio.com/tensorflow/posts/2019-10-03-intro-to-hmc/&#34; target=&#34;_blank&#34;&gt;explanation&lt;/a&gt; is also excellent and less intimidating than the original articles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;ll now go over the code we&amp;rsquo;ll use to run the NUTS sampler. We&amp;rsquo;ll start with explaining &lt;code&gt;trace_fn&lt;/code&gt;, which is not NUTS specific but we&amp;rsquo;ll use it in our &lt;code&gt;run_nuts&lt;/code&gt; function; then we&amp;rsquo;ll cover the &lt;code&gt;run_nuts&lt;/code&gt; function itself.&lt;/p&gt;

&lt;h2 id=&#34;tracing-sampler-statistics&#34;&gt;Tracing sampler statistics&lt;/h2&gt;

&lt;p&gt;TFP allows us to collect various statistics of the sampling procedure, which can be important for diagnosing different kinds of problems with our model. To tell TFP which sampler statistics we actually care about, we need to pass &lt;code&gt;sample_chain&lt;/code&gt; a mysterious looking function called &lt;code&gt;trace_fn&lt;/code&gt;, which takes two arguments: &lt;code&gt;states&lt;/code&gt;, and &lt;code&gt;previous_kernel_results&lt;/code&gt; (or &lt;code&gt;pkr&lt;/code&gt;; took me a while understand what this stands for). In most examples (e.g., in &lt;code&gt;sample_chain&lt;/code&gt;&amp;rsquo;s docs), the &lt;code&gt;states&lt;/code&gt; argument is discarded and some field(s) of the &lt;code&gt;pkr&lt;/code&gt; (which is a &lt;code&gt;collections.namedtuple&lt;/code&gt; object) are returned. To see what are the different kinds of statistics you can extract from &lt;code&gt;previous_kernel_results&lt;/code&gt; for the NUTS sampler, see &lt;a href=&#34;https://github.com/tensorflow/probability/blob/b959b26b7b3eee31711096d140b01cf58768e5e1/tensorflow_probability/python/mcmc/nuts.py#L73&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Following the same notebook linked above, we&amp;rsquo;re extracting the following statistics (most of this came from this PyMC3 &lt;a href=&#34;https://docs.pymc.io/notebooks/sampler-stats.html&#34; target=&#34;_blank&#34;&gt;Sampler Statistics&lt;/a&gt; notebook):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;target_log_prob&lt;/code&gt; - log likelihood of the current state. If you&amp;rsquo;re interested in using ArviZ for model comparison, you need to extract this.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;leapfrogs_taken&lt;/code&gt; - NUTS generates a binary tree during sampling; this is the number of leafs in the tree per iteration. If the tree size is large, this can imply there are strong correlations in the posterior, high curvature &amp;ldquo;funnels&amp;rdquo;, and that &lt;a href=&#34;https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html&#34; target=&#34;_blank&#34;&gt;reparameterization&lt;/a&gt; of the model (for example, moving from a centered to non-centered representation) might be helpful.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;has_divergence&lt;/code&gt; - Whether the trajectory for this sample diverged. See Michael Betancourt&amp;rsquo;s excellent &lt;a href=&#34;https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt; for what are divergences and how you can use them to detect problems with your model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;energy&lt;/code&gt; - The energy at the point in phase-space where the sample was accepted. Can be used to identify posteriors with problematically long tails. See &lt;a href=&#34;https://discourse.pymc.io/t/about-nuts-sampling-and-energy-plot/831&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;log_accept_ratio&lt;/code&gt; - The mean acceptance probability for the tree that generated this sample. This can be compared to the desired target acceptance ratio for diagnosis.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def trace_fn(_, pkr):  
return (
    pkr.inner_results.inner_results.target_log_prob,
    pkr.inner_results.inner_results.leapfrogs_taken,
    pkr.inner_results.inner_results.has_divergence,
    pkr.inner_results.inner_results.energy,
    pkr.inner_results.inner_results.log_accept_ratio
)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;pkr.inner_results.inner_results&lt;/code&gt; part is due to the fact that we&amp;rsquo;re using a kernel-within-a-kernel-within-a-kernel - see below.&lt;/p&gt;

&lt;h2 id=&#34;calling-nuts&#34;&gt;Calling NUTS&lt;/h2&gt;

&lt;p&gt;The code for running the NUTS sampler is somewhat different than the &lt;code&gt;sampleHMC&lt;/code&gt; function we&amp;rsquo;ve used in the previous posts. Here&amp;rsquo;s a helper function for running NUTS that takes a tracing function, a log probability function, a list of initial values and an (optional) list of bijectors, and returns samples and sampler statistics.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n_chains = 10

def run_nuts_template(
    trace_fn,
    target_log_prob_fn,
    inits,
    bijectors_list=None, 
    num_steps=500,
    num_burnin=500,
    n_chains=n_chains):
    
    step_size = np.random.rand(n_chains, 1)*.5 + 1.
    
    if not isinstance(inits, list):
        inits = [inits]
        
    if bijectors_list is None:
        bijectors_list = [tfb.Identity()]*len(inits)

    kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(
        tfp.mcmc.TransformedTransitionKernel(
            inner_kernel=tfp.mcmc.NoUTurnSampler(
                target_log_prob_fn,
                step_size=[step_size]*len(inits)
            ),
            bijector=bijectors_list
        ),
        target_accept_prob=.8,
        num_adaptation_steps=int(0.8*num_burnin),
        step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(
              inner_results=pkr.inner_results._replace(step_size=new_step_size)
          ),
        step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,
        log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio,
    )
    
    res = tfp.mcmc.sample_chain(
        num_results=num_steps,
        num_burnin_steps=num_burnin,
        current_state=inits,
        kernel=kernel,
        trace_fn=trace_fn
    )
    return res
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What have we got here?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;NUTS&lt;/code&gt; kernel is hidden within the &lt;code&gt;TransformedTransitionKernel&lt;/code&gt;, and gets as inputs the &lt;code&gt;target_log_prob_fn&lt;/code&gt; function representing the model we want to sample from, and a list of per-variable &lt;code&gt;step_size&lt;/code&gt; for the NUTS algorithm. TFP&amp;rsquo;s docs recommend these should be of the same order-of-magnitude as the standard deviations of the target distribution; here they&amp;rsquo;re taken to be of order 1, and jittered (this can apparently help with areas of high-curvature of the posterior - see the Stan manual on &lt;a href=&#34;https://mc-stan.org/docs/2_18/reference-manual/hmc-algorithm-parameters.html&#34; target=&#34;_blank&#34;&gt;HMC parameters&lt;/a&gt; for more details).&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;TransformedTransitionKernel&lt;/code&gt; wraps &lt;code&gt;NUTS&lt;/code&gt; in case we&amp;rsquo;re using unconstraining bijectors. For a more in-depth explanation on bijectors, see my previous &lt;a href=&#34;https://adamhaber.github.io/2019/09/02/Varying-Slopes-Models-and-the-CholeskyLKJ-Distribution-in-TensorFlow-Probability.html&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt;. If no bijectors are passed, we&amp;rsquo;re using Identity bijectors.&lt;/li&gt;
&lt;li&gt;This is wrapped by &lt;code&gt;tfp.mcmc.DualAveragingStepSizeAdaptation&lt;/code&gt;, which, as the name suggests, adapts the sampler step size in order to make sampling more efficient. This is the 2nd addition that saves hand-tuning I&amp;rsquo;ve mentioned above. We&amp;rsquo;re setting the number of adaptation steps to be 80% of the number of burnin step, following TFP&amp;rsquo;s &lt;a href=&#34;https://github.com/tensorflow/probability/blob/b959b26b7b3eee31711096d140b01cf58768e5e1/tensorflow_probability/python/mcmc/dual_averaging_step_size_adaptation.py#L113&#34; target=&#34;_blank&#34;&gt;recommendation&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The whole kernel-within-kernel-within-kernel is passed to &lt;code&gt;sample_chain&lt;/code&gt;, along with initial values, the trace function from above, number of burnin steps and number of results.&lt;/p&gt;

&lt;h2 id=&#34;speeding-up-your-sampler-with-xla-compilation&#34;&gt;Speeding up your sampler with XLA compilation&lt;/h2&gt;

&lt;p&gt;We can call &lt;code&gt;run_nuts&lt;/code&gt; as it is and everything will work fine. However, we can use some TFP optimisation tricks to make sampling &lt;em&gt;significantly&lt;/em&gt; faster. Optimizing the sampling function is a one liner:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;run_nuts = partial(run_nuts_template, trace_fn)

run_nuts_opt = tf.function(run_nuts)
run_nuts_defun = tf.function(run_nuts, autograph=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What&amp;rsquo;s this?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We&amp;rsquo;re using &lt;code&gt;functools.partial&lt;/code&gt; to &amp;ldquo;plug in&amp;rdquo; the tracing function. This simply saves writing &lt;code&gt;run_nuts&lt;/code&gt; several times for several tracing functions.&lt;/li&gt;
&lt;li&gt;Decorating the &lt;code&gt;run_nuts&lt;/code&gt; function with &lt;code&gt;tf.function&lt;/code&gt; compiles it into a &lt;code&gt;tf.Graph&lt;/code&gt;, which means faster execution and easy integration with the GPU or TPU. This is done by tracing the TensorFlow operations in &lt;code&gt;run_nuts&lt;/code&gt; and constructing the corresponding &lt;code&gt;tf.Graph&lt;/code&gt;, allowing TensorFlow to optimize and exploit parallelism in the computation defined by &lt;code&gt;run_nuts&lt;/code&gt;. See &lt;a href=&#34;https://www.tensorflow.org/guide/function&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/function&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; if you&amp;rsquo;re interested in learning more.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;autograph=False&lt;/code&gt; is related to how control-flow statements are handled; setting it to &lt;code&gt;False&lt;/code&gt; is the recommendation of the TFP team (we&amp;rsquo;ll benchmark all these below).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can even have a faster version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;run_nuts_defun_xla = tf.function(run_nuts, autograph=False, experimental_compile=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;experimental_compile=True&lt;/code&gt; compiles the whole graph into XLA, which is even faster, as we&amp;rsquo;ll see below. XLA is a domain-specific compiler used by TensorFlow (and jax) to produce highly optimized code and CPU/GPU/TPU compatibility. For learning more about XLA compilation, read &lt;a href=&#34;https://www.tensorflow.org/xla&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; and watch the talk at the bottom of the page.&lt;/p&gt;

&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;

&lt;p&gt;There are two datasets in the original Stan tutorial - we call them &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;new_data&lt;/code&gt;. The first column is the distance (&lt;code&gt;x&lt;/code&gt;) from the hole, the second column is the number of attempts, and the last column is the number of successful putts:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = np.array([[2,1443,1346],
[3,694,577],
[4,455,337],
[5,353,208],
[6,272,149],
[7,256,136],
[8,240,111],
[9,217,69],
[10,200,67],
[11,237,75],
[12,202,52],
[13,192,46],
[14,174,54],
[15,167,28],
[16,201,27],
[17,195,31],
[18,191,33],
[19,147,20],
[20,152,24]])

new_data = np.array([[0.28, 45198, 45183],
[0.97, 183020, 182899],
[1.93, 169503, 168594],
[2.92, 113094, 108953],
[3.93, 73855, 64740],
[4.94, 53659, 41106],
[5.94, 42991, 28205],
[6.95, 37050, 21334],
[7.95, 33275, 16615],
[8.95, 30836, 13503],
[9.95, 28637, 11060],
[10.95, 26239, 9032],
[11.95, 24636, 7687],
[12.95, 22876, 6432],
[14.43, 41267, 9813],
[16.43, 35712, 7196],
[18.44, 31573, 5290],
[20.44, 28280, 4086],
[21.95, 13238, 1642],
[24.39, 46570, 4767],
[28.40, 38422, 2980],
[32.39, 31641, 1996],
[36.39, 25604, 1327],
[40.37, 20366, 834],
[44.38, 15977, 559],
[48.37, 11770, 311],
[52.36, 8708, 231],
[57.25, 8878, 204],
[63.23, 5492, 103],
[69.18, 3087, 35],
[75.19, 1742, 24]])

df = pd.DataFrame(data, columns = [&#39;x&#39;,&#39;n&#39;,&#39;y&#39;])
new_df = pd.DataFrame(new_data, columns = [&#39;x&#39;,&#39;n&#39;,&#39;y&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is how the data looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(df[&#39;x&#39;],df[&#39;y&#39;]/df[&#39;n&#39;],c=&#39;b&#39;,label=&#39;data&#39;)
plt.scatter(new_df[&#39;x&#39;],new_df[&#39;y&#39;]/new_df[&#39;n&#39;],c=&#39;r&#39;,label=&#39;new_data&#39;)
plt.legend()
plt.xlabel(&amp;quot;Distance&amp;quot;)
plt.ylabel(&amp;quot;% of success&amp;quot;)
plt.title(&amp;quot;Original data&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_28_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;model-1&#34;&gt;Model 1&lt;/h1&gt;

&lt;p&gt;The first model is a simple logistic regression. Since the graph above is somewhat skewed and not symmetric around its midpoint (like a sigmoid), you can already guess logistic regression won&amp;rsquo;t be very accurate but why not:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;root = tfd.JointDistributionCoroutine.Root

def golf_logistic():
    a = yield root(tfd.Sample(tfd.Normal(0,1e6),1))
    b = yield root(tfd.Sample(tfd.Normal(0,1e6),1))
    y = yield tfd.Independent(
        tfd.Binomial(
            total_count = tf.cast(df[&#39;n&#39;],dtype),
            logits = a+tf.cast(df[&#39;x&#39;],dtype)*b
        )
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;golf_logistic_jd = tfd.JointDistributionCoroutine(golf_logistic)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that unlike the Stan tutorial, we&amp;rsquo;re using a super-vague but proper prior $\mathcal{N}\left(0,10^6\right)$.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use &lt;code&gt;tfd.JointDistributionCoroutine&lt;/code&gt; to build all 4 models in this post. &lt;code&gt;JointDistributionCoroutine&lt;/code&gt; is a cousin of the more-intuitive &lt;code&gt;JointDistributionSequential&lt;/code&gt; we&amp;rsquo;ve used in previous posts. Instead of a list of &lt;code&gt;tfd.Distribution&lt;/code&gt;-like instances, we&amp;rsquo;re passing a generator that yields a sequence of &lt;code&gt;tfd.Distribution&lt;/code&gt;-like instances. The main advantage is that with &lt;code&gt;Coroutine&lt;/code&gt;&amp;rsquo;s function syntax its easier to express intermediate calculations, compared to &lt;code&gt;Sequential&lt;/code&gt;&amp;rsquo;s list syntax. This will come in handy already in the 2nd model.&lt;/p&gt;

&lt;p&gt;The main differences are that we need to use &lt;code&gt;yield&lt;/code&gt; everytime we&amp;rsquo;re sampling from a distribution, and wrap priors (random variables in the model that don&amp;rsquo;t depend on other random variables) with &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Once we have our model, we feed its &lt;code&gt;log_prob&lt;/code&gt; method together with the observed data into &lt;code&gt;run_nuts&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;golf_logistic_log_prob = lambda *args: golf_logistic_jd.log_prob(args + (tf.cast(df[&#39;y&#39;],dtype),))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see how each of the &lt;code&gt;run_nuts&lt;/code&gt; version is doing:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts(golf_logistic_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 13min 5s, sys: 4.28 s, total: 13min 10s
Wall time: 13min 13s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_opt(golf_logistic_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 29.7 s, sys: 4.13 s, total: 33.8 s
Wall time: 18.7 s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_defun(golf_logistic_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 29 s, sys: 4.04 s, total: 33 s
Wall time: 17.9 s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_defun_xla(golf_logistic_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 6.29 s, sys: 77.9 ms, total: 6.37 s
Wall time: 6.39 s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So &lt;code&gt;tf.function&lt;/code&gt; makes a &lt;em&gt;huge&lt;/em&gt; difference, &lt;code&gt;autograph&lt;/code&gt; True/False doesn&amp;rsquo;t really matter here, and XLA definitely helps.&lt;/p&gt;

&lt;p&gt;Visualizing the results with ArviZ is now straightforward:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trace1 = tfp_trace_to_arviz(res,[&#39;a&#39;,&#39;b&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;az.plot_trace(trace1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_42_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And so is displaying summary statistics:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;az.summary(trace1)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hpd_3%&lt;/th&gt;
      &lt;th&gt;hpd_97%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_mean&lt;/th&gt;
      &lt;th&gt;ess_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;a[0]&lt;/th&gt;
      &lt;td&gt;2.231&lt;/td&gt;
      &lt;td&gt;0.061&lt;/td&gt;
      &lt;td&gt;2.115&lt;/td&gt;
      &lt;td&gt;2.346&lt;/td&gt;
      &lt;td&gt;0.002&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;1017.0&lt;/td&gt;
      &lt;td&gt;1016.0&lt;/td&gt;
      &lt;td&gt;1030.0&lt;/td&gt;
      &lt;td&gt;833.0&lt;/td&gt;
      &lt;td&gt;1.01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;b[0]&lt;/th&gt;
      &lt;td&gt;-0.256&lt;/td&gt;
      &lt;td&gt;0.007&lt;/td&gt;
      &lt;td&gt;-0.269&lt;/td&gt;
      &lt;td&gt;-0.243&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;1294.0&lt;/td&gt;
      &lt;td&gt;1293.0&lt;/td&gt;
      &lt;td&gt;1297.0&lt;/td&gt;
      &lt;td&gt;1320.0&lt;/td&gt;
      &lt;td&gt;1.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Let&amp;rsquo;s see how this logistic regression looks like when we use the mean parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a,b = az.summary(trace1)[&#39;mean&#39;]
plt.plot(df[&#39;x&#39;], [np.exp(x)/(1+np.exp(x)) for x in a+df[&#39;x&#39;]*b],c=&#39;r&#39;,label=&#39;model 1&#39;) 
plt.scatter(df[&#39;x&#39;], df[&#39;y&#39;]/df[&#39;n&#39;],label=&#39;data&#39;) 
plt.legend();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_46_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;model-2&#34;&gt;Model 2&lt;/h1&gt;

&lt;p&gt;Logistic regression is a good start, but we can see it&amp;rsquo;s not doing a very good job at fitting our data. The second model is a major improvement, incorporating the knowledge that this data is describing attempts to insert a small ball into a larger hole; therefore their sizes, the distance between them and the corresponding angles probably matter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;r = (1.68/2)/12 #ball size
R = (4.25/2)/12 #hole size

#threshold angles
df[&#39;th_angle&#39;] = np.arcsin((R-r)/df[&#39;x&#39;])          
new_df[&#39;th_angle&#39;] = np.arcsin((R-r)/new_df[&#39;x&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;normal_cdf = tfb.NormalCDF()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def golf_angle_distance():
    # priors
    sigma = yield root(tfd.Sample(tfd.HalfNormal(1e6),1))
    
    # transformations
    phi = 2*normal_cdf.forward(
        tf.cast(df[&#39;th_angle&#39;],dtype)/sigma
    )-1
    
    # likelihood
    y = yield tfd.Independent(
        tfd.Binomial(
            tf.cast(df[&#39;n&#39;],dtype),
            probs=phi
        )
    )

golf_angle_distance_jd = tfd.JointDistributionCoroutine(golf_angle_distance)
golf_angle_distance_jd_log_prob = lambda *args: golf_angle_distance_jd.log_prob(
    args + (tf.cast(df[&#39;y&#39;], dtype),)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that something like &lt;code&gt;phi&lt;/code&gt; would be difficult to express with &lt;code&gt;Sequential&lt;/code&gt;, but straightforward with &lt;code&gt;Coroutine&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use this model as an opportunity to do something different with our &lt;code&gt;trace_fn&lt;/code&gt;. Stan has this super useful &lt;code&gt;generated quantities&lt;/code&gt; block, in which we can use the sampled parameters to generated various kinds of quantities of interest. We&amp;rsquo;ve mentioned above that &lt;code&gt;trace_fn&lt;/code&gt; takes a &lt;code&gt;state&lt;/code&gt; and a &lt;code&gt;pkr&lt;/code&gt;, and usually discards the first. We&amp;rsquo;ll now modify our &lt;code&gt;trace_fn&lt;/code&gt; so that in each step it takes the current angle (in radians) and converts it to degrees:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def trace_fn_angles(current_state, pkr):  
    return (
        tf.squeeze(current_state[0]*180/np.pi),
        pkr.inner_results.inner_results.target_log_prob,
        pkr.inner_results.inner_results.leapfrogs_taken,
        pkr.inner_results.inner_results.has_divergence,
        pkr.inner_results.inner_results.energy,
        pkr.inner_results.inner_results.log_accept_ratio
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re taking the current state, representing the angle in radians, and convert it to degrees. The &lt;code&gt;tf.squeeze&lt;/code&gt; is here to get rid of a redundant dimension that can cause broadcasting issues later.&lt;/p&gt;

&lt;p&gt;As for timing - the non &lt;code&gt;tf.function&lt;/code&gt;-ed version is so slow that we won&amp;rsquo;t even try. Let&amp;rsquo;s compare the other alternatives:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;run_nuts_angle = partial(run_nuts_template, trace_fn_angles)

run_nuts_opt_angle = tf.function(run_nuts_angle)
run_nuts_defun_angle = tf.function(run_nuts_angle, autograph=False)
run_nuts_defun_xla_angle = tf.function(run_nuts_angle, autograph=False, experimental_compile=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_opt_angle(golf_angle_distance_jd_log_prob, [tf.ones((n_chains,1))], [tfb.Exp()])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 6.18 s, sys: 808 ms, total: 6.99 s
Wall time: 3.96 s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_defun_angle(golf_angle_distance_jd_log_prob, [tf.ones((n_chains,1))], [tfb.Exp()])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 6.06 s, sys: 755 ms, total: 6.82 s
Wall time: 3.93 s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_defun_xla_angle(golf_angle_distance_jd_log_prob, [tf.ones((n_chains,1))], [tfb.Exp()])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 5.29 s, sys: 59.1 ms, total: 5.35 s
Wall time: 5.38 s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No clear winner this time. Let&amp;rsquo;s inspect the trace:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trace2 = tfp_trace_to_arviz(res,[&#39;sigma&#39;],
                            sample_stats_name=[&#39;angle&#39;]+sample_stats_name)
az.plot_trace(trace2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_60_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since we traced the angles using &lt;code&gt;trace_fn_angles&lt;/code&gt;, ArviZ treats it as another sample statistics - it&amp;rsquo;s saved as an &lt;code&gt;xarray.DataArray&lt;/code&gt; and even has its own plotting method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trace2.sample_stats.angle.plot.hist(bins=30);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_62_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I found this nice and cleaner than &lt;code&gt;sns.distplot(res[0][0].numpy().flatten()*180/np.pi)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We print the summary statistics and make sure we&amp;rsquo;re reproducing the Stan tutorial results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;az.summary(trace2)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hpd_3%&lt;/th&gt;
      &lt;th&gt;hpd_97%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_mean&lt;/th&gt;
      &lt;th&gt;ess_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma[0]&lt;/th&gt;
      &lt;td&gt;0.027&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.026&lt;/td&gt;
      &lt;td&gt;0.027&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2074.0&lt;/td&gt;
      &lt;td&gt;2073.0&lt;/td&gt;
      &lt;td&gt;2077.0&lt;/td&gt;
      &lt;td&gt;3648.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&#34;comparing-models-1-2-with-arviz&#34;&gt;Comparing models 1 &amp;amp; 2 with ArviZ&lt;/h2&gt;

&lt;p&gt;ArviZ allows us to conduct model comparison using approximate Leave-One-Out cross validation and WAIC information criteria (read more &lt;a href=&#34;https://mc-stan.org/loo/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; if you&amp;rsquo;re interested). To do so, our ArviZ objects must have a &lt;code&gt;log_likelihood&lt;/code&gt; sampler stats field (this is why we&amp;rsquo;ve included &lt;code&gt;target_log_prob&lt;/code&gt; in our tracing function):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trace1.sample_stats.log_likelihood.shape, trace2.sample_stats.log_likelihood.shape 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((10, 500), (10, 500))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Model comparison is now as easy as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;az.compare({&#39;Model 1&#39;:trace1, &#39;Model 2&#39;:trace2})
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;rank&lt;/th&gt;
      &lt;th&gt;waic&lt;/th&gt;
      &lt;th&gt;p_waic&lt;/th&gt;
      &lt;th&gt;d_waic&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;se&lt;/th&gt;
      &lt;th&gt;dse&lt;/th&gt;
      &lt;th&gt;warning&lt;/th&gt;
      &lt;th&gt;waic_scale&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Model 2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;203.973&lt;/td&gt;
      &lt;td&gt;0.521524&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.20886e-14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;deviance&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Model 1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;424.486&lt;/td&gt;
      &lt;td&gt;1.12655&lt;/td&gt;
      &lt;td&gt;220.513&lt;/td&gt;
      &lt;td&gt;1.30697e-48&lt;/td&gt;
      &lt;td&gt;1.10443e-14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;deviance&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The models are ranked from best to worst, so Model 2 (not surprisingly) outperforms Model 1. For more info about model comparison, how to read this table and its visualization using &lt;code&gt;az.plot_compare&lt;/code&gt;, see &lt;a href=&#34;https://docs.pymc.io/notebooks/model_comparison.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;model-3&#34;&gt;Model 3&lt;/h1&gt;

&lt;p&gt;The authors aren&amp;rsquo;t satisfied with the fit of Model 2 and incorporate yet another piece of golf knowledge - other than shooting the ball at the right angle, you need to shoot it to the right distance, as well. They describe how this can be modelled, and proceed to fit the model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distance_tol = 3.
overshot = 1.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def golf_angle_distance_2():
    # priors
    sigma_angle = yield root(tfd.Sample(tfd.HalfNormal(1),1))
    sigma_distance = yield root(tfd.Sample(tfd.HalfNormal(1),1))

    # transformations
    p_angle = 2 * normal_cdf.forward(
        tf.cast(new_df[&#39;th_angle&#39;],dtype)/sigma_angle
    )-1
    
    p_dist = normal_cdf.forward(
        (distance_tol-overshot)/(tf.cast(new_df[&#39;x&#39;]+overshot,dtype)*sigma_distance)
    ) - \
    normal_cdf.forward(
        (-overshot)/(tf.cast(new_df[&#39;x&#39;]+overshot,dtype)*sigma_distance)
    )
    
    # likelihood
    y = yield tfd.Independent(
        tfd.Binomial(
            tf.cast(new_df[&#39;n&#39;],dtype),
            probs=p_angle*p_dist
        )
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;golf_angle_distance_2_jd = tfd.JointDistributionCoroutine(golf_angle_distance_2)
golf_angle_distance_2_jd_log_prob = lambda *args: golf_angle_distance_2_jd.log_prob(
    args + (tf.cast(new_df[&#39;y&#39;], dtype),)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_opt(golf_angle_distance_2_jd_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1))],
                              bijectors_list=[tfb.Exp(), tfb.Exp()])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 23min 51s, sys: 4min 30s, total: 28min 21s
Wall time: 10min 13s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_defun(golf_angle_distance_2_jd_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1))],
                              bijectors_list=[tfb.Exp(), tfb.Exp()])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 18min 56s, sys: 3min 51s, total: 22min 48s
Wall time: 8min 11s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_defun_xla(golf_angle_distance_2_jd_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1))],
                              bijectors_list=[tfb.Exp(), tfb.Exp()])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 1min 57s, sys: 38.1 s, total: 2min 35s
Wall time: 1min 6s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, XLA makes ~8x difference in speed, which is incredible for an inherently iterative process!&lt;/p&gt;

&lt;p&gt;As for the trace:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trace3 = tfp_trace_to_arviz(res,[&#39;sigma_angle&#39;,&#39;sigma_distance&#39;])
az.plot_trace(trace3);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_80_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This isn&amp;rsquo;t what you want to see when inspecting a trace&amp;hellip; The chains haven&amp;rsquo;t mixed at all. Surprisingly, the Stan version had divergent transitions here, while we did not:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trace3.sample_stats.diverging.sum()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;xarray.DataArray &#39;diverging&#39; ()&amp;gt;
array(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;model-4&#34;&gt;Model 4&lt;/h1&gt;

&lt;p&gt;In the 4th model, instead of using a binomial likelihood, we&amp;rsquo;re using a normal approximation of the binomial distribution and adding an additional noise term (read the original for a more in-depth explanation):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def golf_angle_distance_3(): 
    # priors
    sigma_angle = yield root(tfd.Sample(tfd.HalfNormal(1), 1))
    sigma_distance = yield root(tfd.Sample(tfd.HalfNormal(1), 1))
    sigma_y = yield root(tfd.Sample(tfd.HalfNormal(1), 1))
    
    # transformations
    p_angle = 2 * normal_cdf.forward(
      tf.cast(new_df[&#39;th_angle&#39;],dtype)/sigma_angle
    ) - 1
    p_dist = normal_cdf.forward(
      (distance_tol-overshot)/(tf.cast(new_df[&#39;x&#39;]+overshot,dtype)*sigma_distance)
    ) - \
    normal_cdf.forward(
      (-overshot)/(tf.cast(new_df[&#39;x&#39;]+overshot,dtype)*sigma_distance)
    )
    p = p_dist*p_angle
    
    # likelihood
    probs = yield tfd.Independent(
        tfd.Normal(p, tf.sqrt(p*(1-p)/(tf.cast(new_df[&#39;n&#39;],dtype))+sigma_y**2))
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;golf_angle_distance_3_jd = tfd.JointDistributionCoroutine(golf_angle_distance_3)

golf_angle_distance_3_jd_log_prob = lambda *args: golf_angle_distance_3_jd.log_prob(
    args + (tf.cast(new_df[&#39;y&#39;]/new_df[&#39;n&#39;], dtype),)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_opt(golf_angle_distance_3_jd_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1)),tf.ones((n_chains,1))],
                 bijectors_list=[tfb.Exp()]*3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 3min 45s, sys: 45.6 s, total: 4min 30s
Wall time: 1min 39s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_defun(golf_angle_distance_3_jd_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1)),tf.ones((n_chains,1))],
                 bijectors_list=[tfb.Exp()]*3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 4min 2s, sys: 48.4 s, total: 4min 50s
Wall time: 1min 46s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
res = run_nuts_defun_xla(golf_angle_distance_3_jd_log_prob, [tf.ones((n_chains,1)), tf.ones((n_chains,1)),tf.ones((n_chains,1))],
                 bijectors_list=[tfb.Exp()]*3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 15.8 s, sys: 177 ms, total: 16 s
Wall time: 16.1 s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, XLA is simply an incredible speed up of the sampling function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trace4 = tfp_trace_to_arviz(res,[&#39;sigma_angle&#39;,&#39;sigma_distance&#39;,&#39;sigma_y&#39;])
az.plot_trace(trace4);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_91_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This looks much better:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;az.summary(trace4)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hpd_3%&lt;/th&gt;
      &lt;th&gt;hpd_97%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_mean&lt;/th&gt;
      &lt;th&gt;ess_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma_angle[0]&lt;/th&gt;
      &lt;td&gt;0.018&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.018&lt;/td&gt;
      &lt;td&gt;0.018&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3349.0&lt;/td&gt;
      &lt;td&gt;3348.0&lt;/td&gt;
      &lt;td&gt;3330.0&lt;/td&gt;
      &lt;td&gt;3283.0&lt;/td&gt;
      &lt;td&gt;1.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma_distance[0]&lt;/th&gt;
      &lt;td&gt;0.080&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.078&lt;/td&gt;
      &lt;td&gt;0.083&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3178.0&lt;/td&gt;
      &lt;td&gt;3169.0&lt;/td&gt;
      &lt;td&gt;3169.0&lt;/td&gt;
      &lt;td&gt;2313.0&lt;/td&gt;
      &lt;td&gt;1.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma_y[0]&lt;/th&gt;
      &lt;td&gt;0.003&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.002&lt;/td&gt;
      &lt;td&gt;0.004&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;662.0&lt;/td&gt;
      &lt;td&gt;662.0&lt;/td&gt;
      &lt;td&gt;656.0&lt;/td&gt;
      &lt;td&gt;675.0&lt;/td&gt;
      &lt;td&gt;1.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We&amp;rsquo;ll extract the mean parameters from the summary, and plot the resulting function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mean_sigma_angle, mean_sigma_dist, mean_sigma_y = az.summary(trace4)[&#39;mean&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;normal_cdf = tfb.NormalCDF()

p_angle = 2 * normal_cdf.forward(
    tf.cast(new_df[&#39;th_angle&#39;],dtype)/mean_sigma_angle
)-1

p_dist = normal_cdf.forward(
    (distance_tol-overshot)/(tf.cast(new_df[&#39;x&#39;]+overshot,dtype)*mean_sigma_dist)
) - \
normal_cdf.forward(
    (-overshot)/(tf.cast(new_df[&#39;x&#39;]+overshot,dtype)*mean_sigma_dist)
)
mean_p = (p_dist*p_angle).numpy()
std_p = (tf.sqrt(mean_p*(1-mean_p)/(tf.cast(new_df[&#39;n&#39;],dtype))+mean_sigma_y**2)).numpy()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(new_df[&#39;x&#39;],new_df[&#39;y&#39;]/new_df[&#39;n&#39;],label=&#39;data&#39;)
plt.errorbar(new_df[&#39;x&#39;],mean_p,yerr=std_p,c=&#39;k&#39;,label=&#39;model 4&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_97_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is, by all means, a very impressive fit. And the errorbars are there, they&amp;rsquo;re just too small we can&amp;rsquo;t actually see them. Definitely not bad for a 3 parameters model.&lt;/p&gt;

&lt;h1 id=&#34;comparison-summary&#34;&gt;Comparison summary&lt;/h1&gt;

&lt;p&gt;This is a summary of the different timings:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Model 1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Model 2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Model 3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Model 4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;without tf.function&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;793&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;tf.function, autograph=True&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.96&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;613&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;tf.function, autograph=False&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;3.93&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;491&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;106&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;tf.function + XLA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;6.39&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.38&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;66&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;16&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So XLA compilation is a clear winner. Running the same models with 256 chains instead of 10 (very easy to do with TFP) gives qualitatively similar results.&lt;/p&gt;

&lt;h1 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h1&gt;

&lt;p&gt;In this post we&amp;rsquo;ve tried to replicate Stan&amp;rsquo;s awesome golf tutorial. Along the way, we saw how to work with the new NUTS kernel, how to speed it up using &lt;code&gt;tf.function&lt;/code&gt; and XLA compilation, how to use the &lt;code&gt;trace_fn&lt;/code&gt; to generated quantities of interest, and how to use ArviZ to handle all the plotting for us.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
