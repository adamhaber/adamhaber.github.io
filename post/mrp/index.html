<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Adam Haber">

  
  
  
    
  
  <meta name="description" content="TL;DR We&rsquo;ll:
 Learn an interesting method for generalizing inferences from a biased sample to a population of interest See why prior predictive checks are great Implement a simple mixed-effects model in TFP  Intro This post is a TFP port of Lauren Kennedy and Jonah Gabry&rsquo;s excellent MRP with rstanarm vignette. It describes a very interesting statistical method for generalizing inferences from a biased sample to a population of interest.">

  
  <link rel="alternate" hreflang="en-us" href="https://adamhaber.github.io/post/mrp/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-143367176-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-143367176-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://adamhaber.github.io/post/mrp/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@_adam_haber">
  <meta property="twitter:creator" content="@_adam_haber">
  
  <meta property="og:site_name" content="Adam Haber">
  <meta property="og:url" content="https://adamhaber.github.io/post/mrp/">
  <meta property="og:title" content="Mr. P meets TFP - mixed effects model with post-stratification in TensorFlow Probability | Adam Haber">
  <meta property="og:description" content="TL;DR We&rsquo;ll:
 Learn an interesting method for generalizing inferences from a biased sample to a population of interest See why prior predictive checks are great Implement a simple mixed-effects model in TFP  Intro This post is a TFP port of Lauren Kennedy and Jonah Gabry&rsquo;s excellent MRP with rstanarm vignette. It describes a very interesting statistical method for generalizing inferences from a biased sample to a population of interest."><meta property="og:image" content="https://adamhaber.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://adamhaber.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-11-18T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-11-18T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://adamhaber.github.io/post/mrp/"
  },
  "headline": "Mr. P meets TFP - mixed effects model with post-stratification in TensorFlow Probability",
  
  "datePublished": "2019-11-18T00:00:00Z",
  "dateModified": "2019-11-18T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Adam Haber"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Adam Haber",
    "logo": {
      "@type": "ImageObject",
      "url": "https://adamhaber.github.io/img/icon-512.png"
    }
  },
  "description": "TL;DR We\u0026rsquo;ll:\n Learn an interesting method for generalizing inferences from a biased sample to a population of interest See why prior predictive checks are great Implement a simple mixed-effects model in TFP  Intro This post is a TFP port of Lauren Kennedy and Jonah Gabry\u0026rsquo;s excellent MRP with rstanarm vignette. It describes a very interesting statistical method for generalizing inferences from a biased sample to a population of interest."
}
</script>

  

  


  


  





  <title>Mr. P meets TFP - mixed effects model with post-stratification in TensorFlow Probability | Adam Haber</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    
    
      <a class="navbar-brand" href="/">Adam Haber</a>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  



















<div class="article-container pt-3">
  <h1>Mr. P meets TFP - mixed effects model with post-stratification in TensorFlow Probability</h1>

  

  
  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Nov 18, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    20 min read
  </span>
  

  
  
  

  
  

</div>

  














  
</div>



  <div class="article-container">

    <div class="article-style">
      

<h2 id="tl-dr">TL;DR</h2>

<p>We&rsquo;ll:</p>

<ul>
<li>Learn an interesting method for generalizing inferences from a biased sample to a population of interest</li>
<li>See why prior predictive checks are great</li>
<li>Implement a simple mixed-effects model in TFP</li>
</ul>

<h1 id="intro">Intro</h1>

<p>This post is a TFP port of Lauren Kennedy and Jonah Gabry&rsquo;s excellent <a href="http://mc-stan.org/rstanarm/articles/mrp.html" target="_blank">MRP with rstanarm</a> vignette. It describes a very interesting statistical method for generalizing inferences from a biased sample to a population of interest. The method is called multilevel regression with poststratification, or MRP if you prefer acronyms, or Mister P if you prefer statisticians jokes. Along the way, we&rsquo;ll see why prior predictive checks are so nice and important, how to implement a mixed-effect model in TFP, and how to make predictions for smaller sub-populations.</p>

<p>I chose to port the vignette because the problem MRP address - generalizing from a biased sample to a population - is so prevalent and important, that knowing what are the possible tools to handle it seemed valuable. I found that porting models from one language to another is an excellent way to learn the model, the problem, and the languages themselves, so it&rsquo;s kind of a win-win-win and publishing it might also help others so why not.</p>

<p>I strongly recommend reading the original vignette; the people who wrote it are much more knowledgeable than I am about this subject, and I also chose to focus on slightly different things so they&rsquo;re not 100% overlapping. At the end of this post you can find links for further reading.</p>

<h3 id="imports-and-helper-functions-data-generation">Imports and helper functions - data generation</h3>

<pre><code class="language-python">from collections import namedtuple
import numpy as np
import itertools as it
import pandas as pd
from scipy.special import expit as inv_logit
from scipy.stats import sem
import seaborn as sns
import matplotlib.pyplot as plt
</code></pre>

<pre><code class="language-python">np.random.seed(98)

sns.set_palette(&quot;muted&quot;)
params = {
    'legend.fontsize': 'x-large',
    'figure.figsize': (9, 6),
    'axes.labelsize': 'x-large',
    'axes.titlesize':'x-large',
    'xtick.labelsize':'x-large',
    'ytick.labelsize':'x-large'
}
plt.rcParams.update(params)
%config InlineBackend.figure_format = 'retina'
</code></pre>

<h1 id="the-data">The data</h1>

<p>The data we&rsquo;ll work with is simulated data; this has the obvious advantage that we know the ground truth so we&rsquo;ll be able to assess just how well our method generalizes to the population. The data describes the proportion of the population who would choose to adopt a cat over a dog, given the opportunity. Our outcome variable in this example is binary (cat/dog), but MRP is not restricted to such outcomes and can be used for discrete outcomes with more than two values, as well as continuous outcomes.</p>

<p>These are the variables we&rsquo;ll be working with:</p>

<pre><code class="language-python">sex = range(2)
eth = range(3)
age = range(7)
income = range(3)
state = range(50)
</code></pre>

<p>They&rsquo;re all categorical; we use zero-based indexing to enumerate them (instead of calling them &lsquo;Male&rsquo;, &lsquo;Female&rsquo; etc) because it&rsquo;ll make all the indexing gymnastics in the actual implementation somewhat simpler.</p>

<p><code>poststrat</code> is a dataframe containing all $2\times3\times7\times3\times50=6300$ possible combinations of these variables:</p>

<pre><code class="language-python">poststrat = pd.DataFrame(
    list(it.product(sex, eth, age, income, state)),
    columns=[&quot;sex&quot;, &quot;eth&quot;, &quot;age&quot;, &quot;income&quot;, &quot;state&quot;],
)
poststrat.sample(5)
</code></pre>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">sex</th>
<th align="right">eth</th>
<th align="right">age</th>
<th align="right">income</th>
<th align="right">state</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">4675</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">25</td>
</tr>

<tr>
<td align="right">1620</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">2</td>
<td align="right">20</td>
</tr>

<tr>
<td align="right">1749</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">49</td>
</tr>

<tr>
<td align="right">1141</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">41</td>
</tr>

<tr>
<td align="right">2460</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">10</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">poststrat.shape
</code></pre>

<pre><code>(6300, 5)
</code></pre>

<p>Below are the different proportions of the different variables <em>in the population</em>. For example, 20% of the population are in the first age group, 10% are in the second, etc. For each combination of variables we&rsquo;ll compute the number of people that share this specific combination by multiplying the total number of people in the population (assumed to be 250 million) with the different probabilities (this means we&rsquo;re assuming the joint probability distribution factorizes, that is - that the different variables are independent).</p>

<pre><code class="language-python">p_age = np.array([0.2, 0.1, 0.2, 0.2, 0.10, 0.1, 0.1])
p_sex = np.array([0.52, 0.48])
p_eth = np.array([0.5, 0.2, 0.3])
p_income = np.array([0.50, 0.35, 0.15])
p_state_tmp = np.random.uniform(low=10, high=20, size=50)
p_state = np.array(p_state_tmp / p_state_tmp.sum())

poststrat[&quot;N&quot;] = (
    250e6
    * p_sex[poststrat[&quot;sex&quot;]]
    * p_eth[poststrat[&quot;eth&quot;]]
    * p_age[poststrat[&quot;age&quot;]]
    * p_income[poststrat[&quot;income&quot;]]
    * p_state[poststrat[&quot;state&quot;]]
)
</code></pre>

<p>We also assume that different groups have different probabilities of being included in the sample; in a way, that&rsquo;s the entire point (if all groups had the same probability of being included in the sample then the sample was representative of the population). There&rsquo;s a baseline probability of being in the sample, but it cancels out in the weighted average; what determines who is in our sample is <code>p_response_weighted</code>, which is <code>p_response</code> weighted by the number of people in each group:</p>

<pre><code class="language-python">p_response_baseline = 0.01
p_response_sex = np.array([2, 0.8]) / 2.8
p_response_eth = np.array([1, 1.2, 2.5]) / 4.7 
p_response_age = np.array([1, 0.4, 1, 1.5, 3, 5, 7]) / 18.9
p_response_inc = np.array([1, 0.9, 0.8]) / 2.7
p_response_state = np.random.beta(a=1, b=1, size=50)
p_response_state = p_response_state / p_response_state.sum()

p_response = (
    p_response_baseline
    * p_response_sex[poststrat[&quot;sex&quot;]]
    * p_response_eth[poststrat[&quot;eth&quot;]]
    * p_response_age[poststrat[&quot;age&quot;]]
    * p_response_inc[poststrat[&quot;income&quot;]]
    * p_response_state[poststrat[&quot;state&quot;]]
)

p_response_weighted = poststrat[&quot;N&quot;] * p_response / (poststrat[&quot;N&quot;] * p_response).sum()
</code></pre>

<p>We now sample 1200 individuals from the entire population. This means we&rsquo;re actually sampling rows from our <code>poststrat</code> dataframe with different probabilities given by <code>p_response_weighted</code>:</p>

<pre><code class="language-python">n = 1200
people = np.random.choice(
    np.arange(poststrat.shape[0]), size=n, replace=True, p=p_response_weighted
)
sample = poststrat.drop(&quot;N&quot;, axis=1).iloc[people].reset_index()
sample.sample(5)
</code></pre>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">index</th>
<th align="right">sex</th>
<th align="right">eth</th>
<th align="right">age</th>
<th align="right">income</th>
<th align="right">state</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">520</td>
<td align="right">2141</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">41</td>
</tr>

<tr>
<td align="right">287</td>
<td align="right">2626</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">26</td>
</tr>

<tr>
<td align="right">870</td>
<td align="right">2591</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">41</td>
</tr>

<tr>
<td align="right">69</td>
<td align="right">1517</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">17</td>
</tr>

<tr>
<td align="right">169</td>
<td align="right">4104</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right">1</td>
<td align="right">4</td>
</tr>
</tbody>
</table>

<p>Now we&rsquo;re getting to the thing we&rsquo;ll actually measure in the sample (and then try to generalize to the population) - cat preference. Below are the coefficients of a regression model that determines the log-odds of cat preference, $\log\frac{P(\text{prefers cats})}{P(\text{prefers dogs})}$ for each group in the population. We&rsquo;ll use these coefficients to compute the actual probability of cats preference for each group:</p>

<pre><code class="language-python">coef_sex = np.array([0, -0.3])
coef_eth = np.array([0, 0.6, 0.9])
coef_age = np.array([0, -0.2, -0.3, 0.4, 0.5, 0.7, 0.8, 0.9])
coef_income = np.array([0, -0.2, 0.6])
coef_state = np.insert(np.random.normal(0, 1, 49).round(1), 0, 0)
coef_age_sex = np.vstack(
    [
        np.array([0, 0.1, 0.23, 0.3, 0.43, 0.5, 0.6]),
        np.array([0, -0.1, -0.23, -0.5, -0.43, -0.5, -0.6]),
    ]
).T
</code></pre>

<pre><code class="language-python">true_pop = poststrat.drop(&quot;N&quot;, axis=1)
true_pop[&quot;cat_pref&quot;] = inv_logit(
    coef_sex[true_pop[&quot;sex&quot;]]
    + coef_eth[true_pop[&quot;eth&quot;]]
    + coef_age[true_pop[&quot;age&quot;]]
    + coef_income[true_pop[&quot;income&quot;]]
    + coef_state[true_pop[&quot;state&quot;]]
    + coef_age_sex[true_pop[&quot;age&quot;], true_pop[&quot;sex&quot;]]
)
true_pop.sample(5)
</code></pre>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">sex</th>
<th align="right">eth</th>
<th align="right">age</th>
<th align="right">income</th>
<th align="right">state</th>
<th align="right">cat_pref</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">6124</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="right">24</td>
<td align="right">0.71095</td>
</tr>

<tr>
<td align="right">177</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">27</td>
<td align="right">0.549834</td>
</tr>

<tr>
<td align="right">5755</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">0.331812</td>
</tr>

<tr>
<td align="right">2102</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0.802184</td>
</tr>

<tr>
<td align="right">3931</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">31</td>
<td align="right">0.524979</td>
</tr>
</tbody>
</table>

<p>We now use the computed probabilities to determine, for each individual in our sample, whether she&rsquo;s a cats person or a dogs person. Note that this is still the fake data generation part; we&rsquo;re not modelling anything yet.</p>

<pre><code class="language-python">sample[&quot;cat_pref&quot;] = np.random.binomial(n=1, p=true_pop[&quot;cat_pref&quot;][people], size=n)
sample.head()
</code></pre>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">index</th>
<th align="right">sex</th>
<th align="right">eth</th>
<th align="right">age</th>
<th align="right">income</th>
<th align="right">state</th>
<th align="right">cat_pref</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">0</td>
<td align="right">671</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">21</td>
<td align="right">1</td>
</tr>

<tr>
<td align="right">1</td>
<td align="right">2141</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">41</td>
<td align="right">1</td>
</tr>

<tr>
<td align="right">2</td>
<td align="right">906</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>

<tr>
<td align="right">3</td>
<td align="right">3062</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">1</td>
<td align="right">12</td>
<td align="right">1</td>
</tr>

<tr>
<td align="right">4</td>
<td align="right">6043</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">43</td>
<td align="right">1</td>
</tr>
</tbody>
</table>

<p>Just to get a glimpse of the problem Mr. P is trying to solve, the sample mean is:</p>

<pre><code class="language-python">sample[&quot;cat_pref&quot;].mean()
</code></pre>

<pre><code>0.7083333333333334
</code></pre>

<p>While the true mean in the population (which is a weighted sum of the per-group probabilities and the group sizes) is:</p>

<pre><code class="language-python">true_pop_pref = sum(true_pop[&quot;cat_pref&quot;] * poststrat[&quot;N&quot;]) / sum(poststrat[&quot;N&quot;])
true_pop_pref
</code></pre>

<pre><code>0.5941253009200917
</code></pre>

<p>So our sample overestimates cats-lovin&rsquo; in the population by 18% - people who like cats also like taking surveys.</p>

<h2 id="visualizations">Visualizations</h2>

<p>To get a better understanding of the problem (unrepresentativeness of the sample), we&rsquo;ll plot some summary statistics and see how they differ:</p>

<pre><code class="language-python">f, ax = plt.subplots(1, 4, figsize=(12, 3))

pd.DataFrame(
    dict(pop=pd.Series(p_age), sample=(sample.age.value_counts().sort_index() / n))
).plot(kind=&quot;bar&quot;, ax=ax[0], title=&quot;age&quot;)
pd.DataFrame(
    dict(pop=pd.Series(p_eth), sample=(sample.eth.value_counts().sort_index() / n))
).plot(kind=&quot;bar&quot;, ax=ax[1], legend=False, title=&quot;ethnicity&quot;)
pd.DataFrame(
    dict(
        pop=pd.Series(p_income), sample=(sample.income.value_counts().sort_index() / n)
    )
).plot(kind=&quot;bar&quot;, ax=ax[2], legend=False, title=&quot;income&quot;)
pd.DataFrame(
    dict(pop=pd.Series(p_sex), sample=(sample.sex.value_counts().sort_index() / n))
).plot(kind=&quot;bar&quot;, ax=ax[3], legend=False, title=&quot;sex&quot;)
plt.tight_layout()
</code></pre>

<p><img src="output_33_0.png" alt="png" /></p>

<p>At least by eyeballing the charts, the differences seem substantial; for example, if there&rsquo;s a big difference in cats preference between males and females, we expect to see a substantial difference between the cats preference in the sample and in the population.</p>

<p>We can also plot how cats preference changes between different groups <em>within</em> our sample - for example, is there a difference in cats preference between different age groups? (yes there is)</p>

<pre><code class="language-python">f, axes = plt.subplots(1, 4, figsize=(12, 3), sharey=True)
for key, ax in zip([&quot;age&quot;, &quot;eth&quot;, &quot;income&quot;, &quot;sex&quot;], axes):
    sample.groupby(key)[&quot;cat_pref&quot;].agg(dict(mean=np.mean, std=sem)).reset_index().plot(
        kind=&quot;bar&quot;, x=key, y=&quot;mean&quot;, yerr=&quot;std&quot;, ax=ax, legend=False
    )
    plt.ylim(0, 1)
plt.tight_layout()
</code></pre>

<p><img src="output_36_1.png" alt="png" /></p>

<h1 id="the-model">The model</h1>

<p>We now turn to the MR part of MRP - the multilevel regression part. More specifically, we&rsquo;ll build a Bayesian multilevel logistic regression model of cats preference. Even more specifically, we&rsquo;ll build what&rsquo;s called a &ldquo;mixed
effects&rdquo; model. Mixed effects models are one of those places that, at least for me, the statisticians terminology is <em>extremely</em> confusing; it also seems to be inconsistent between different academic fields.  I usually find it easier to look at the actual model specification to understand what&rsquo;s going on:</p>

<p>For each group $j\in\left[1,&hellip;,6300\right]$ we model the probability of cats preference as</p>

<p>$$
\begin{align}
\theta_j &amp; = logit^{-1}(
\alpha +
X_{j}\beta
+ \alpha_{\rm state[j]}^{\rm state}
+ \alpha_{\rm age[j]}^{\rm age}
+ \alpha_{\rm eth[j]}^{\rm eth}
+ \alpha_{\rm inc[j]}^{\rm inc}
) \\<br />
\alpha_{\rm state[j]}^{\rm state} &amp; \sim N(0,\sigma^{\rm state}) \\<br />
\alpha_{\rm age[j]}^{\rm age} &amp; \sim N(0,\sigma^{\rm age})\\<br />
\alpha_{\rm eth[j]}^{\rm eth} &amp; \sim N(0,\sigma^{\rm eth})\\<br />
\alpha_{\rm inc[j]}^{\rm inc} &amp;\sim N(0,\sigma^{\rm inc}) \\<br />
\sigma^{\rm state} &amp; \sim {\rm HalfNormal}(1) \\<br />
\sigma^{\rm age} &amp; \sim {\rm HalfNormal}(1) \\<br />
\sigma^{\rm eth} &amp; \sim {\rm HalfNormal}(1) \\<br />
\sigma^{\rm income} &amp; \sim {\rm HalfNormal}(1) \\<br />
\beta &amp; \sim N(0,2.5) \\<br />
\alpha &amp; \sim N(0,10) \\<br />
\end{align}
$$</p>

<p>We&rsquo;ve seen expressions like $\alpha_{\rm state[j]}^{\rm state}$ when we&rsquo;ve implemented <a href="https://adamhaber.github.io/2019/07/08/A-Tutorial-on-Varying-Intercepts-Models-with-TensorFlow-Probability.html" target="_blank">varying intercepts models</a>. What makes this a &ldquo;mixed effects&rdquo; models is that $\beta$ is the same $\beta$ for all groups, while the different $\alpha^*$-s vary between groups. I&rsquo;m sure there are subtleties and nuances that this doesn&rsquo;t capture, but for me this is a simple-to-read, simple-to-implement explanation of mixed effects models.</p>

<p>As for the model itself:</p>

<ul>
<li>$X$ is a (binary) design matrix that holds indicators for sex, age and sex-age interactions - we&rsquo;ll construct it in a second.</li>
<li>$\alpha$ is an intercept term.</li>
<li>$\beta$ is a coefficient vector.</li>
<li>The different $\alpha^*$-s are per-group varying intercepts.</li>
<li>The different $\sigma^*$-s are hyperpriors for variation between groups.</li>
</ul>

<p>The priors on $\alpha,\beta$ are rstanarm&rsquo;s default priors; I couldn&rsquo;t find rstanarm&rsquo;s default prior on the $\sigma^*$ so I chose to use a halfnormal(1) prior.</p>

<p>Our design matrix $X$ will represent a one-hot-encoded representation of the sampled individuals sex, age, and sex-age interaction term. Here&rsquo;s how it looks like:</p>

<pre><code class="language-python">factors = pd.get_dummies(sample[[&quot;sex&quot;, &quot;age&quot;]].astype(&quot;category&quot;)).drop(
    [&quot;sex_0&quot;, &quot;age_0&quot;], axis=1
)
interactions = pd.DataFrame(
    factors.drop(&quot;sex_1&quot;, axis=1).values * factors[&quot;sex_1&quot;].values[:, None],
    columns=[f&quot;sex_1*age_{i+1}&quot; for i in range(6)],
)
features = pd.concat([factors, interactions], axis=1)
features.head()
</code></pre>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">sex_1</th>
<th align="right">age_1</th>
<th align="right">age_2</th>
<th align="right">age_3</th>
<th align="right">age_4</th>
<th align="right">age_5</th>
<th align="right">age_6</th>
<th align="right">sex_1*age_1</th>
<th align="right">sex_1*age_2</th>
<th align="right">sex_1*age_3</th>
<th align="right">sex_1*age_4</th>
<th align="right">sex_1*age_5</th>
<th align="right">sex_1*age_6</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>

<tr>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
</tbody>
</table>

<pre><code class="language-python">features.shape
</code></pre>

<pre><code>(1200, 13)
</code></pre>

<p>To make TF shape issues simpler, we convert it to a numpy array and transpose it:</p>

<pre><code class="language-python">features = features.values.T
</code></pre>

<h3 id="imports-and-helper-functions-inference">Imports and helper functions - inference</h3>

<pre><code class="language-python">import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow_probability import distributions as tfd
from tensorflow_probability import bijectors as tfb
import arviz as az
</code></pre>

<pre><code class="language-python">n_chains = 4
dtype = tf.float32
</code></pre>

<pre><code class="language-python">def step_size_setter_fn(pkr, new_step_size):
    return pkr._replace(
        inner_results=pkr.inner_results._replace(step_size=new_step_size)
    )
</code></pre>

<pre><code class="language-python">factors = pd.get_dummies(sample[[&quot;sex&quot;, &quot;age&quot;]].astype(&quot;category&quot;)).drop(
    [&quot;sex_0&quot;, &quot;age_0&quot;], axis=1
)
interactions = pd.DataFrame(
    factors.drop(&quot;sex_1&quot;, axis=1).values * factors[&quot;sex_1&quot;].values[:, None],
    columns=[f&quot;sex_1*age_{i}&quot; for i in range(6)],
)
features = pd.concat([factors, interactions], axis=1).values.T
</code></pre>

<pre><code class="language-python">def trace_fn(current_samp, pkr):

    return (
        pkr.inner_results.inner_results.target_log_prob,
        pkr.inner_results.inner_results.leapfrogs_taken,
        pkr.inner_results.inner_results.has_divergence,
        pkr.inner_results.inner_results.energy,
        pkr.inner_results.inner_results.log_accept_ratio,
    )
</code></pre>

<pre><code class="language-python">@tf.function(experimental_compile=True)
def run_nuts(target_log_prob_fn, initial_states, bijectors_list):
    step_sizes = [1e-2 * tf.ones_like(i) for i in initial_states]
    kernel = tfp.mcmc.TransformedTransitionKernel(
        tfp.mcmc.nuts.NoUTurnSampler(target_log_prob_fn, step_size=step_sizes),
        bijector=bijectors_list,
    )

    kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(
        kernel,
        target_accept_prob=tf.cast(0.8, dtype=dtype),
        num_adaptation_steps=800,
        step_size_setter_fn=step_size_setter_fn,
        step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,
        log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio,
    )

    # Sampling from the chain.
    mcmc_trace, pkr = tfp.mcmc.sample_chain(
        num_results=1000,
        num_burnin_steps=1000,
        current_state=[
            bijector.forward(state)
            for bijector, state in zip(bijectors_list, initial_states)
        ],
        kernel=kernel,
        trace_fn=trace_fn,
    )

    return mcmc_trace, pkr
</code></pre>

<pre><code class="language-python"># using pymc3 naming conventions, with log_likelihood instead of lp so that ArviZ can compute loo and waic
sample_stats_name = [
    &quot;log_likelihood&quot;,
    &quot;tree_size&quot;,
    &quot;diverging&quot;,
    &quot;energy&quot;,
    &quot;mean_tree_accept&quot;,
]


def tfp_trace_to_arviz(tfp_trace, var_names=None, sample_stats_name=sample_stats_name):

    samps, trace = tfp_trace
    if var_names is None:
        var_names = [&quot;var &quot; + str(x) for x in range(len(samps))]

    sample_stats = {k: v.numpy().T for k, v in zip(sample_stats_name, trace)}
    posterior = {
        name: tf.transpose(samp, [1, 0, 2]).numpy()
        for name, samp in zip(var_names, samps)
    }
    return az.from_dict(posterior=posterior, sample_stats=sample_stats)
</code></pre>

<p>For more details about calling TFP&rsquo;s NUTS sampler, and the helper functions defined above, see <a href="https://adamhaber.github.io/2019/10/21/Bayesian-golf-puttings,-NUTS,-and-optimizing-your-sampling-function-with-TensorFlow-Probability.html" target="_blank">here</a>.</p>

<h2 id="first-implemetation">First implemetation</h2>

<p>We now turn to implement the whole model in TFP. Since there aren&rsquo;t many complicated intermediate calculations, a <code>JointDistributionSequential</code> is a reasonable choice for implementing the model. For a more detailed explanation on the different <code>JointDistribution</code> alternatives, see <a href="https://adamhaber.github.io/2019/10/21/Bayesian-golf-puttings,-NUTS,-and-optimizing-your-sampling-function-with-TensorFlow-Probability.html#model-1" target="_blank">this post</a>.</p>

<pre><code class="language-python">model = tfd.JointDistributionSequential(
    [
        tfd.HalfNormal(1),  # sigma_state
        lambda sigma_state: tfd.Sample(tfd.Normal(0, sigma_state), sample_shape=50),
        tfd.HalfNormal(1),  # sigma_eth
        lambda sigma_eth: tfd.Sample(tfd.Normal(0, sigma_eth), sample_shape=3),
        tfd.HalfNormal(1),  # sigma_income
        lambda sigma_income: tfd.Sample(tfd.Normal(0, sigma_income), sample_shape=3),
        tfd.HalfNormal(1),  # sigma_age
        lambda sigma_age: tfd.Sample(tfd.Normal(0, sigma_age), sample_shape=7),
        tfd.Normal(0, 10),  # intercept
        tfd.Sample(tfd.Normal(0, 2.5), sample_shape=13),  # coeffs
        lambda coeffs, intercept, coef_age, sigma_age, coef_income, sigma_income, coef_eth, sigma_eth, coef_state: tfd.Independent(
            tfd.Binomial(
                total_count=1,
                logits=intercept[:, tf.newaxis]
                + coeffs @ tf.cast(features, tf.float32)
                + tf.squeeze(
                    tf.gather(coef_age, tf.cast(sample[&quot;age&quot;], tf.int32), axis=-1)
                )
                + tf.squeeze(
                    tf.gather(coef_income, tf.cast(sample[&quot;income&quot;], tf.int32), axis=-1)
                )
                + tf.squeeze(
                    tf.gather(coef_eth, tf.cast(sample[&quot;eth&quot;], tf.int32), axis=-1)
                )
                + tf.squeeze(
                    tf.gather(coef_state, tf.cast(sample[&quot;state&quot;], tf.int32), axis=-1)
                ),
            ),
            reinterpreted_batch_ndims=1,
        ),
    ]
)
</code></pre>

<p>The model description isn&rsquo;t short, but it doesn&rsquo;t contain anything we haven&rsquo;t covered in previous posts. Let&rsquo;s call <code>.sample</code> and <code>.log_prob</code> just to make sure everything works:</p>

<pre><code class="language-python">[s.shape for s in model.sample(n_chains)]
</code></pre>

<pre><code>[TensorShape([4]),
 TensorShape([4, 50]),
 TensorShape([4]),
 TensorShape([4, 3]),
 TensorShape([4]),
 TensorShape([4, 3]),
 TensorShape([4]),
 TensorShape([4, 7]),
 TensorShape([4]),
 TensorShape([4, 13]),
 TensorShape([4, 1200])]
</code></pre>

<pre><code class="language-python">model.log_prob(model.sample(n_chains))
</code></pre>

<pre><code>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([-121.06135 ,   21.950146,  -69.38415 , -224.08742 ], dtype=float32)&gt;
</code></pre>

<p>So our model technically works&hellip; but does it makes sense?</p>

<h2 id="prior-predictive-checks">Prior predictive checks</h2>

<p>Prior predictive checks are an extremely valuable technique to assess your model and your priors, before seeing any data. To learn more about PPCs (horrible acronym as the first P can also stand for <em>posterior</em>), I highly recommend Michael Betancourt&rsquo;s <a href="https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html" target="_blank">principled bayesian workflow</a> case study.</p>

<p>Anyway, let&rsquo;s generate samples from our model, and use the samples to compute the logits (the linear expression within the <code>inv_logit</code> function):</p>

<pre><code class="language-python">inits = model.sample(n_chains)
</code></pre>

<pre><code class="language-python">coeffs, intercept, coef_age, _, coef_income, _, coef_eth, _, coef_state = inits[1:10][
    ::-1
]
</code></pre>

<pre><code class="language-python">logits = (
    intercept[:, tf.newaxis]
    + coeffs @ tf.cast(features, tf.float32)
    + tf.squeeze(tf.gather(coef_age, tf.cast(sample[&quot;age&quot;], tf.int32), axis=-1))
    + tf.squeeze(tf.gather(coef_income, tf.cast(sample[&quot;income&quot;], tf.int32), axis=-1))
    + tf.squeeze(tf.gather(coef_eth, tf.cast(sample[&quot;eth&quot;], tf.int32), axis=-1))
    + tf.squeeze(tf.gather(coef_state, tf.cast(sample[&quot;state&quot;], tf.int32), axis=-1))
)
</code></pre>

<p>Each chain gives us 1200 different numbers - the log-odds for cat preference for our 1200 sampled individuals. Let&rsquo;s plot these four histograms:</p>

<pre><code class="language-python">for i, l in enumerate(logits):
    sns.distplot(l, bins=30, label=f&quot;from chain {i}&quot;)
plt.legend()
plt.xlabel(&quot;${\\rm logit}\\left(\\theta_j\\right)$&quot;)
lim = plt.xlim();
</code></pre>

<p><img src="output_67_0.png" alt="png" /></p>

<p>Note that it&rsquo;s OK that each color (each chain) is multimodal - this just means that we&rsquo;re inferring different &ldquo;types&rdquo; of cats preference across groups.</p>

<p>The problem with what we got is the <em>scale</em> - having ${\rm logit}\left(\theta_j\right)=-15$ means $\theta_j=0.000000003&hellip;$ which doesn&rsquo;t really makes sense, even for a group that <em>really</em> likes dogs. This implies that our priors are way too diffuse, the normal(0,10) being the primary suspect. So let&rsquo;s make everything normal(0,1) and do this again:</p>

<h1 id="same-likelihood-better-priors">Same likelihood, better priors</h1>

<pre><code class="language-python">model = tfd.JointDistributionSequential(
    [
        tfd.HalfNormal(1),  # sigma_state
        lambda sigma_state: tfd.Sample(tfd.Normal(0, sigma_state), sample_shape=50),
        tfd.HalfNormal(1),  # sigma_eth
        lambda sigma_eth: tfd.Sample(tfd.Normal(0, sigma_eth), sample_shape=3),
        tfd.HalfNormal(1),  # sigma_income
        lambda sigma_income: tfd.Sample(tfd.Normal(0, sigma_income), sample_shape=3),
        tfd.HalfNormal(1),  # sigma_age
        lambda sigma_age: tfd.Sample(tfd.Normal(0, sigma_age), sample_shape=7),
        tfd.Normal(0, 1),  # intercept
        tfd.Sample(tfd.Normal(0, 1), sample_shape=13),  # coeffs
        lambda coeffs, intercept, coef_age, a, coef_income, b, coef_eth, c, coef_state: tfd.Independent(
            tfd.Binomial(
                total_count=1,
                logits=intercept[:, tf.newaxis]
                + coeffs @ tf.cast(features, tf.float32)
                + tf.squeeze(
                    tf.gather(coef_age, tf.cast(sample[&quot;age&quot;], tf.int32), axis=-1)
                )
                + tf.squeeze(
                    tf.gather(coef_income, tf.cast(sample[&quot;income&quot;], tf.int32), axis=-1)
                )
                + tf.squeeze(
                    tf.gather(coef_eth, tf.cast(sample[&quot;eth&quot;], tf.int32), axis=-1)
                )
                + tf.squeeze(
                    tf.gather(coef_state, tf.cast(sample[&quot;state&quot;], tf.int32), axis=-1)
                ),
            ),
            reinterpreted_batch_ndims=1,
        ),
    ]
)
</code></pre>

<pre><code class="language-python">inits = model.sample(n_chains)
</code></pre>

<pre><code class="language-python">coeffs, intercept, coef_age, _, coef_income, _, coef_eth, _, coef_state = inits[1:10][
    ::-1
]
</code></pre>

<pre><code class="language-python">logits = (
    intercept[:, tf.newaxis]
    + coeffs @ tf.cast(features, tf.float32)
    + tf.squeeze(tf.gather(coef_age, tf.cast(sample[&quot;age&quot;], tf.int32), axis=-1))
    + tf.squeeze(tf.gather(coef_income, tf.cast(sample[&quot;income&quot;], tf.int32), axis=-1))
    + tf.squeeze(tf.gather(coef_eth, tf.cast(sample[&quot;eth&quot;], tf.int32), axis=-1))
    + tf.squeeze(tf.gather(coef_state, tf.cast(sample[&quot;state&quot;], tf.int32), axis=-1))
)
</code></pre>

<pre><code class="language-python">for i, l in enumerate(logits):
    sns.distplot(l, bins=30, label=f&quot;from chain {i}&quot;)
plt.legend()
plt.xlabel(&quot;${\\rm logit}\\left(\\theta_j\\right)$&quot;)
plt.xlim(*lim);
</code></pre>

<p><img src="output_74_0.png" alt="png" /></p>

<p>This makes much more sense. The variance between groups is still there but it doesn&rsquo;t spread across several order of magnitude (that is, with this prior it&rsquo;s no longer plausible that some groups love cats 10 million times more than other groups). This seems like a good starting point.</p>

<p>Note that the overly wide priors are also very problematic, inference wise - running the same notebook with the first model returns all sorts of sampling problems (divergent transitions, bad mixing, random seed dependence etc) while the 2nd, more informed version does not.</p>

<h2 id="getting-the-shapes-right">Getting the shapes right</h2>

<p>This is, by far, the hardest thing for me when building a probablistic model with TFP. Knowing where to put <code>[...,],  tf.newaxis</code> or <code>[None,]</code> requires some trial and error - here are some checks to verify we got this right (after <em>a lot</em> of failed attempts and some help from Junpeng Lao):</p>

<p>First, we want to make sure the model can evaluate the log probability of its own samples, and that we get <code>n_chains</code> different numbers:</p>

<pre><code class="language-python">model.log_prob(inits)
</code></pre>

<pre><code>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([-619.5229 , -520.8101 , -460.97076, -609.21765], dtype=float32)&gt;
</code></pre>

<p>Second, we want to make sure that all shapes of the different parameters in our samples are as we expect, which basically should be the number of chains in the first axis and the shape of whatever it is we&rsquo;re sampling in the rest - or nothing, if it&rsquo;s just a scalar:</p>

<pre><code class="language-python">[s.shape for s in inits]
</code></pre>

<pre><code>[TensorShape([4]),
 TensorShape([4, 50]),
 TensorShape([4]),
 TensorShape([4, 3]),
 TensorShape([4]),
 TensorShape([4, 3]),
 TensorShape([4]),
 TensorShape([4, 7]),
 TensorShape([4]),
 TensorShape([4, 13]),
 TensorShape([4, 1200])]
</code></pre>

<p>The main thing to look out for here are redundant extra dimensions (for example, <code>TensorShape([4, 1])</code> instead of <code>TensorShape([4])</code> - these will almost always cause broadcasting issues.</p>

<p>Next, we want to add an extra axis for the data we condition on. Again, this is for broadcasting purposes - we want to make sure <code>tf</code> &ldquo;replicates&rdquo; the data across different chains.</p>

<pre><code class="language-python">tf.cast(sample[&quot;cat_pref&quot;], tf.float32)[tf.newaxis, ...].shape
</code></pre>

<pre><code>TensorShape([1, 1200])
</code></pre>

<p>Finally, the <code>log_prob</code> function closure - we want to make sure our <code>log_prob</code> function gets as inputs all the different parameters, concatenates them with the data we&rsquo;re conditioning on, and then uses the original model <code>log_prob</code> function to evaluate; practically, we want to verify that if we pass all the parameters (<em>without</em> the conditioning data), we get <code>n_chains</code> different numbers:</p>

<pre><code class="language-python">lp = lambda *x: model.log_prob(
    list(x) + [tf.cast(sample[&quot;cat_pref&quot;], tf.float32)[tf.newaxis, ...]]
)
lp(*inits[:-1])
</code></pre>

<pre><code>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1315.5156, -1436.2715, -1799.2578, -1901.1874], dtype=float32)&gt;
</code></pre>

<h2 id="inference">Inference</h2>

<p>With sensible priors and TFP shape issues dealt with, we can proceed with actually runnning the sampler.</p>

<pre><code class="language-python">inits = [
    tf.random.uniform(s.shape, -2, 2, tf.float32, name=&quot;initializer&quot;) for s in inits
]
</code></pre>

<pre><code class="language-python">trace, kr = run_nuts(
    lp,
    inits[:-1],
    bijectors_list=[
        tfb.Exp(),
        tfb.Identity(),
        tfb.Exp(),
        tfb.Identity(),
        tfb.Exp(),
        tfb.Identity(),
        tfb.Exp(),
        tfb.Identity(),
        tfb.Identity(),
        tfb.Identity(),
    ],
)
</code></pre>

<p><em>Always</em> check your TF shapes:</p>

<pre><code class="language-python">[s.shape for s in trace]
</code></pre>

<pre><code>[TensorShape([1000, 4]),
 TensorShape([1000, 4, 50]),
 TensorShape([1000, 4]),
 TensorShape([1000, 4, 3]),
 TensorShape([1000, 4]),
 TensorShape([1000, 4, 3]),
 TensorShape([1000, 4]),
 TensorShape([1000, 4, 7]),
 TensorShape([1000, 4]),
 TensorShape([1000, 4, 13])]
</code></pre>

<p>This looks good; for arviz intergration purposes, we&rsquo;ll add an extra axis for the parameters whose tensor shape is <code>TensorShape([1000, 4])</code>, and then call our <code>tfp_trace_to_arviz</code> helper function:</p>

<pre><code class="language-python">trace_ex = [s[..., tf.newaxis] if len(s.shape) == 2 else s for s in trace]
az_trace = tfp_trace_to_arviz((trace_ex, kr))
</code></pre>

<pre><code class="language-python">az.summary(az_trace).head(5)
</code></pre>

<table>
<thead>
<tr>
<th align="left"></th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">hpd_3%</th>
<th align="right">hpd_97%</th>
<th align="right">mcse_mean</th>
<th align="right">mcse_sd</th>
<th align="right">ess_mean</th>
<th align="right">ess_sd</th>
<th align="right">ess_bulk</th>
<th align="right">ess_tail</th>
<th align="right">r_hat</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">var 0[0]</td>
<td align="right">1.056</td>
<td align="right">0.153</td>
<td align="right">0.779</td>
<td align="right">1.338</td>
<td align="right">0.004</td>
<td align="right">0.003</td>
<td align="right">1785</td>
<td align="right">1771</td>
<td align="right">1806</td>
<td align="right">2668</td>
<td align="right">1</td>
</tr>

<tr>
<td align="left">var 1[0]</td>
<td align="right">0.016</td>
<td align="right">0.607</td>
<td align="right">-1.173</td>
<td align="right">1.155</td>
<td align="right">0.011</td>
<td align="right">0.009</td>
<td align="right">2822</td>
<td align="right">2235</td>
<td align="right">2843</td>
<td align="right">2538</td>
<td align="right">1</td>
</tr>

<tr>
<td align="left">var 1[1]</td>
<td align="right">0.025</td>
<td align="right">0.505</td>
<td align="right">-0.997</td>
<td align="right">0.927</td>
<td align="right">0.008</td>
<td align="right">0.008</td>
<td align="right">3978</td>
<td align="right">1785</td>
<td align="right">4013</td>
<td align="right">2918</td>
<td align="right">1</td>
</tr>

<tr>
<td align="left">var 1[2]</td>
<td align="right">0.477</td>
<td align="right">0.463</td>
<td align="right">-0.362</td>
<td align="right">1.38</td>
<td align="right">0.007</td>
<td align="right">0.006</td>
<td align="right">4207</td>
<td align="right">2794</td>
<td align="right">4232</td>
<td align="right">2521</td>
<td align="right">1</td>
</tr>

<tr>
<td align="left">var 1[3]</td>
<td align="right">-0.36</td>
<td align="right">0.764</td>
<td align="right">-1.896</td>
<td align="right">1.008</td>
<td align="right">0.017</td>
<td align="right">0.013</td>
<td align="right">2102</td>
<td align="right">1745</td>
<td align="right">2132</td>
<td align="right">1908</td>
<td align="right">1</td>
</tr>
</tbody>
</table>

<p>Sampling diagnostics look good; we have no divergent transitions, and $\hat{R}$ values are all close to 1:</p>

<pre><code class="language-python">az.summary(az_trace)[&quot;r_hat&quot;].describe()
</code></pre>

<pre><code>count    81.0
mean      1.0
std       0.0
min       1.0
25%       1.0
50%       1.0
75%       1.0
max       1.0
Name: r_hat, dtype: float64
</code></pre>

<p>We won&rsquo;t go down the model-diagnostics-rabbit-hole now; we&rsquo;re here to learn about Mister P.</p>

<h2 id="p-part">P part</h2>

<p>So far we&rsquo;ve defined, critisized and fitted a multilevel logisitic regression model. Now comes the poststratification part. Poststratification is a technical and intimidating word; it basically means &ldquo;adjusting the inferences from my sample to the population by using additional knowledge about proportions in the population&rdquo;. To do so, we&rsquo;ll:</p>

<ol>
<li>Compute a design matrix $X$ for the population.</li>
<li>Use our 4000 sampled parameters to compute 4000 different logits for each group in the population. This will yield a 4000x6300 matrix.</li>
<li>For each row (representing a single draw from our posterior), we&rsquo;ll compute the population mean as a weighted sum of per-group cat preference and group&rsquo;s size. This will give us a vector of 4000 numbers.</li>

<li><p>The mean of these 4000 numbers will be our estimate for the population mean.</p>

<pre><code class="language-python">post_factors = pd.get_dummies(poststrat[[&quot;sex&quot;, &quot;age&quot;]].astype(&quot;category&quot;)).drop(
[&quot;sex_0&quot;, &quot;age_0&quot;], axis=1
)
post_interactions = pd.DataFrame(
post_factors.drop(&quot;sex_1&quot;, axis=1).values * post_factors[&quot;sex_1&quot;].values[:, None],
columns=[f&quot;sex_1*age_{i}&quot; for i in range(6)],
)
post_features = pd.concat([post_factors, post_interactions], axis=1).values.T
</code></pre>

<pre><code class="language-python">intercept = trace[8]
coeffs = trace[9]
coef_age = trace[7]
coef_income = trace[5]
coef_eth = trace[3]
coef_state = trace[1]

logits = (
intercept[..., tf.newaxis]
+ coeffs @ tf.cast(post_features, tf.float32)
+ tf.gather(trace[7], tf.cast(poststrat[&quot;age&quot;], tf.int32), axis=-1)
+ tf.gather(coef_income, tf.cast(poststrat[&quot;income&quot;], tf.int32), axis=-1)
+ tf.gather(coef_eth, tf.cast(poststrat[&quot;eth&quot;], tf.int32), axis=-1)
+ tf.gather(coef_state, tf.cast(poststrat[&quot;state&quot;], tf.int32), axis=-1)
)
posterior_prob = inv_logit(logits)
posterior_prob = posterior_prob.reshape(-1, 6300)
posterior_prob.shape
</code></pre>

<p>(4000, 6300)</p>

<pre><code class="language-python">poststrat_prob = posterior_prob @ poststrat[&quot;N&quot;][:, None] / poststrat[&quot;N&quot;].sum()
poststrat_prob.shape
</code></pre>

<p>(4000, 1)</p></li>
</ol>

<p>So how good is MRP? We plot the histogram of our 4000 different estimates of the population mean, together with the estimate from the sample (dashed line) and the true mean:</p>

<pre><code class="language-python">sns.distplot(poststrat_prob, bins=100)
plt.axvline(true_pop_pref, label=&quot;population mean&quot;, lw=3, c=&quot;k&quot;)
plt.axvline(sample[&quot;cat_pref&quot;].mean(), label=&quot;sample mean&quot;, lw=3, ls=&quot;--&quot;, c=&quot;k&quot;)
plt.legend();
</code></pre>

<p><img src="output_106_0.png" alt="png" /></p>

<p>You can see that the posterior mean is much closer to the true mean - so MRP definitely helps!</p>

<h1 id="estimates-for-states">Estimates for states</h1>

<p>The nice thing about having a model is that we can use it to answer all sorts of different questions. For example, we can repeat the analysis we just did and estimate per-state means. We&rsquo;re still computing the design matrix, logits etc as before but we&rsquo;re constraining ourselves to one state at a time. For each state, we&rsquo;ll compute the model&rsquo;s mean and standard deviations, together with the true mean and the sample mean:</p>

<pre><code class="language-python">state_data = namedtuple(
    &quot;state_data&quot;,
    [
        &quot;state&quot;,
        &quot;model_state_sd&quot;,
        &quot;model_state_pref&quot;,
        &quot;sample_state_pref&quot;,
        &quot;true_state_pref&quot;,
        &quot;N&quot;,
    ],
)
states_data = []

for i in range(50):
    state_features = np.squeeze(post_features[:, np.where(poststrat.state == i)])
    state_poststrat = poststrat.query(f&quot;state=={i}&quot;)
    logits = (
        intercept[..., tf.newaxis]
        + coeffs @ tf.cast(state_features, tf.float32)
        + tf.gather(
            trace[7],
            tf.cast(state_poststrat.query(f&quot;state=={i}&quot;)[&quot;age&quot;], tf.int32),
            axis=-1,
        )
        + tf.gather(
            coef_income,
            tf.cast(state_poststrat.query(f&quot;state=={i}&quot;)[&quot;income&quot;], tf.int32),
            axis=-1,
        )
        + tf.gather(
            coef_eth,
            tf.cast(state_poststrat.query(f&quot;state=={i}&quot;)[&quot;eth&quot;], tf.int32),
            axis=-1,
        )
        + tf.gather(
            coef_state,
            tf.cast(state_poststrat.query(f&quot;state=={i}&quot;)[&quot;state&quot;], tf.int32),
            axis=-1,
        )
    )
    posterior_prob = inv_logit(logits)
    posterior_prob = posterior_prob.reshape(-1, state_features.shape[1])
    state_poststrat_prob = (
        posterior_prob
        @ state_poststrat.query(f&quot;state=={i}&quot;)[&quot;N&quot;][:, None]
        / state_poststrat[&quot;N&quot;].sum()
    )
    states_data.append(
        state_data(
            i,
            state_poststrat_prob.std(),
            state_poststrat_prob.mean(),
            sample.query(f&quot;state=={i}&quot;)[&quot;cat_pref&quot;].mean(),
            np.sum(true_pop.query(f&quot;state=={i}&quot;)[&quot;cat_pref&quot;] * state_poststrat[&quot;N&quot;])
            / np.sum(state_poststrat[&quot;N&quot;]),
            sample.query(f&quot;state=={i}&quot;).shape[0],
        )
    )
</code></pre>

<pre><code class="language-python">state_df = pd.DataFrame(states_data)
state_df.head()
</code></pre>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">state</th>
<th align="right">model_state_sd</th>
<th align="right">model_state_pref</th>
<th align="right">sample_state_pref</th>
<th align="right">true_state_pref</th>
<th align="right">N</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.116844</td>
<td align="right">0.580393</td>
<td align="right">0.75</td>
<td align="right">0.596565</td>
<td align="right">12</td>
</tr>

<tr>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0.0956771</td>
<td align="right">0.583293</td>
<td align="right">0.7</td>
<td align="right">0.658961</td>
<td align="right">20</td>
</tr>

<tr>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">0.0817888</td>
<td align="right">0.669766</td>
<td align="right">0.823529</td>
<td align="right">0.69817</td>
<td align="right">34</td>
</tr>

<tr>
<td align="right">3</td>
<td align="right">3</td>
<td align="right">0.145332</td>
<td align="right">0.505946</td>
<td align="right">0.6</td>
<td align="right">0.553341</td>
<td align="right">5</td>
</tr>

<tr>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">0.0768275</td>
<td align="right">0.493726</td>
<td align="right">0.611111</td>
<td align="right">0.443913</td>
<td align="right">36</td>
</tr>
</tbody>
</table>

<p>Graphically, this is how this looks like:</p>

<pre><code class="language-python">f, ax = plt.subplots(figsize=(6, 6))
state_df.plot(
    x=&quot;true_state_pref&quot;,
    y=&quot;model_state_pref&quot;,
    yerr=&quot;model_state_sd&quot;,
    ax=ax,
    kind=&quot;scatter&quot;,
    label=&quot;Model&quot;,
)
state_df.plot(
    x=&quot;true_state_pref&quot;,
    y=&quot;sample_state_pref&quot;,
    ax=ax,
    kind=&quot;scatter&quot;,
    c=&quot;C1&quot;,
    label=&quot;Sample&quot;,
)
ax.plot([0, 1], [0, 1], c=&quot;k&quot;)
f.tight_layout()
plt.ylabel(&quot;Cat preference&quot;);
</code></pre>

<p><img src="output_113_1.png" alt="png" /></p>

<p>We can see that the model predictions of state-wise preferences (blue dots) are closer to the identity line compared to the orange dots (sample per-state mean preferences).</p>

<p>Another interesting thing to see is how the model uncertainty (quantified by the standard deviation of the model predictions, per state) is related to sample size; we can see that the model is more confident (lower std) for states with higher N, which is what we would expect:</p>

<pre><code class="language-python">plt.scatter(state_df[&quot;N&quot;], state_df[&quot;model_state_sd&quot;])
</code></pre>

<p><img src="output_116_1.png" alt="png" /></p>

<h1 id="summary-and-further-reading">Summary and further reading</h1>

<p>This post was a code-oriented introduction to MRP, which is a very interesting technique that nicely leverages the built in advantages of multilevel models. We&rsquo;ve also seen how taking a package&rsquo;s priors for granted is not always a good idea, and how prior predictive checks can help us calibrate our priors and our beliefs.</p>

<p>In case you want to learn more, other than Lauren and Jonah&rsquo;s vignette, these are all excellent reads:</p>

<ul>
<li>Austin Rochford&rsquo;s <a href="https://austinrochford.com/posts/2017-07-09-mrpymc3.html" target="_blank">MRPyMC3</a> tutorial</li>
<li>Andrew Gelman&rsquo;s post about Mister P&rsquo;s <a href="https://statmodeling.stat.columbia.edu/2013/10/09/mister-p-whats-its-secret-sauce/" target="_blank">secret sauce</a>. Somewhat more technical and perhaps more political-science specific, but still interesting and relevant.</li>
<li>Dan Simpson&rsquo;s post on <a href="https://statmodeling.stat.columbia.edu/2019/08/22/multilevel-structured-regression-and-post-stratification/" target="_blank">structured priors</a> for MRP; this is somewhat more advanced, but Dan&rsquo;s posts are always fun to read.</li>
</ul>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/tfp/">TFP</a>
  
  <a class="badge badge-light" href="/tags/multilevel-models/">Multilevel Models</a>
  
  <a class="badge badge-light" href="/tags/bayesian-models/">Bayesian Models</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://adamhaber.github.io/post/mrp/&amp;text=Mr.%20P%20meets%20TFP%20-%20mixed%20effects%20model%20with%20post-stratification%20in%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://adamhaber.github.io/post/mrp/&amp;t=Mr.%20P%20meets%20TFP%20-%20mixed%20effects%20model%20with%20post-stratification%20in%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Mr.%20P%20meets%20TFP%20-%20mixed%20effects%20model%20with%20post-stratification%20in%20TensorFlow%20Probability&amp;body=https://adamhaber.github.io/post/mrp/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://adamhaber.github.io/post/mrp/&amp;title=Mr.%20P%20meets%20TFP%20-%20mixed%20effects%20model%20with%20post-stratification%20in%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Mr.%20P%20meets%20TFP%20-%20mixed%20effects%20model%20with%20post-stratification%20in%20TensorFlow%20Probability%20https://adamhaber.github.io/post/mrp/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://adamhaber.github.io/post/mrp/&amp;title=Mr.%20P%20meets%20TFP%20-%20mixed%20effects%20model%20with%20post-stratification%20in%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hud138a96a47a9e2657537fb19ee618cb6_16021_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://adamhaber.github.io/">Adam Haber</a></h5>
      <h6 class="card-subtitle">Computational Neuroscience PhD Student</h6>
      <p class="card-text">Interested in probabilistic programming, computational statistics, statistical physics and programming languages.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#mailto:adamhaber@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/_adam_haber" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/adamhaber" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>






<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>







<div class="article-widget content-widget-hr">
  <h3>Related</h3>
  <ul>
    
    <li><a href="/post/varying-slopes/">Varying Slopes Models and the CholeskyLKJ distribution in TensorFlow Probability</a></li>
    
    <li><a href="/post/varying-intercepts/">A Tutorial on Varying Intercepts Models with TensorFlow Probability</a></li>
    
    <li><a href="/post/nuts/">Bayesian golf puttings, NUTS, and optimizing your sampling function with TensorFlow Probability</a></li>
    
    <li><a href="/post/survival-analysis/">Survival analysis, censoring and hacking the log_prob in TensorFlow Probability</a></li>
    
  </ul>
</div>



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.96cf4c3dc37ea60dbbd03c13a455f1f7.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
