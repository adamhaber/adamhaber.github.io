<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Adam Haber">

  
  
  
    
  
  <meta name="description" content="TL;DR Covariance matrices allow us to capture parameter correlations in multivariate hierarchical models; sampling these using Hamiltonian Monte Carlo in Tensorflow Probability can be tricky and confusing; this post is about some of the math involved and how to get this right.
Intro Hierarchical models allow us to account for variations between different groups in our data. Let&rsquo;s say that, for some reason, we have different groups of tadpoles in different tanks and we want to model per-tank survival rates.">

  
  <link rel="alternate" hreflang="en-us" href="https://adamhaber.github.io/post/varying-slopes/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-143367176-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-143367176-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://adamhaber.github.io/post/varying-slopes/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@_adam_haber">
  <meta property="twitter:creator" content="@_adam_haber">
  
  <meta property="og:site_name" content="Adam Haber">
  <meta property="og:url" content="https://adamhaber.github.io/post/varying-slopes/">
  <meta property="og:title" content="Varying Slopes Models and the CholeskyLKJ distribution in TensorFlow Probability | Adam Haber">
  <meta property="og:description" content="TL;DR Covariance matrices allow us to capture parameter correlations in multivariate hierarchical models; sampling these using Hamiltonian Monte Carlo in Tensorflow Probability can be tricky and confusing; this post is about some of the math involved and how to get this right.
Intro Hierarchical models allow us to account for variations between different groups in our data. Let&rsquo;s say that, for some reason, we have different groups of tadpoles in different tanks and we want to model per-tank survival rates."><meta property="og:image" content="https://adamhaber.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://adamhaber.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-02T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-09-02T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://adamhaber.github.io/post/varying-slopes/"
  },
  "headline": "Varying Slopes Models and the CholeskyLKJ distribution in TensorFlow Probability",
  
  "datePublished": "2019-09-02T00:00:00Z",
  "dateModified": "2019-09-02T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Adam Haber"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Adam Haber",
    "logo": {
      "@type": "ImageObject",
      "url": "https://adamhaber.github.io/img/icon-512.png"
    }
  },
  "description": "TL;DR Covariance matrices allow us to capture parameter correlations in multivariate hierarchical models; sampling these using Hamiltonian Monte Carlo in Tensorflow Probability can be tricky and confusing; this post is about some of the math involved and how to get this right.\nIntro Hierarchical models allow us to account for variations between different groups in our data. Let\u0026rsquo;s say that, for some reason, we have different groups of tadpoles in different tanks and we want to model per-tank survival rates."
}
</script>

  

  


  


  





  <title>Varying Slopes Models and the CholeskyLKJ distribution in TensorFlow Probability | Adam Haber</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    
    
      <a class="navbar-brand" href="/">Adam Haber</a>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  



















<div class="article-container pt-3">
  <h1>Varying Slopes Models and the CholeskyLKJ distribution in TensorFlow Probability</h1>

  

  
  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Sep 2, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    15 min read
  </span>
  

  
  
  

  
  

</div>

  














  
</div>



  <div class="article-container">

    <div class="article-style">
      

<h4 id="tl-dr">TL;DR</h4>

<p>Covariance matrices allow us to capture parameter correlations in multivariate hierarchical models; sampling these using Hamiltonian Monte Carlo in Tensorflow Probability can be tricky and confusing; this post is about some of the math involved and how to get this right.</p>

<h1 id="intro">Intro</h1>

<p>Hierarchical models allow us to account for variations between different groups in our data. Let&rsquo;s say that, for some reason, we have <a href="https://adamhaber.github.io/2019/07/08/A-Tutorial-on-Varying-Intercepts-Models-with-TensorFlow-Probability.html" target="_blank">different groups of tadpoles</a> in different tanks and we want to model per-tank survival rates. Varying intercepts models allow us to fit different models to different tanks, while pooling together information <em>between</em> tanks. The tanks are somewhat different (they&rsquo;re not the same tank), so we allow their parameters to vary; but they&rsquo;re also similar (they&rsquo;re all tanks with tadpoles, not oranges or ships), so we can do some &ldquo;transfer learning&rdquo; between tanks.</p>

<p>Varying <em>intercepts</em> are already very powerful models. However, in many (most?) situations, the models we fit have more than just an intercept. Let&rsquo;s say we have 3 groups in our data, and we want to fit a simple linear model for each group, but also to share information between groups. Each model has two parameters (a slope and an intercept), and we allow these to <em>vary</em>. We can also allow them to <em>covary</em>. For example, if higher slopes usually go with lower intercepts, we want to know that, and use that to improve our estimation of both.</p>

<p>To capture this covariance amongst parameters, we&rsquo;re going to need a covariance matrix.</p>

<h1 id="the-lkj-prior">The LKJ prior</h1>

<p>Every 2x2 covariance matrix can be decomposed as a product of a diagonal matrix of standard deviations $\sigma_\alpha,\sigma_\beta$ with a correlation matrix $\Sigma$, in the following form (same holds for higher dimensions):</p>

<p>$$\mathbb{S} = \left(\begin{smallmatrix} \sigma_\alpha &amp; 0 \\\ 0 &amp; \sigma_\beta \end{smallmatrix}\right) \cdot \Sigma \cdot \left(\begin{smallmatrix} \sigma_\alpha &amp; 0 \\\ 0 &amp; \sigma_\beta \end{smallmatrix}\right)$$</p>

<p>The decomposition is conceptually useful - it&rsquo;s usually easier to think about the variances (which are single-parameter properties, and depend on things like unit of measurement and typical scale) separately from the correlation structure (a pairwise property). Technically, putting a prior on the variances isn&rsquo;t very hard - we just need to make sure the variables are non-negative.</p>

<p>A priori, it&rsquo;s not obvious how to put a prior on correlation matrices. We can&rsquo;t sample each matrix element by itself; correlation matrices have to be postitive definite, so their elements are somewhat &ldquo;entangled&rdquo; - the value in the $\left[i,j\right]$-th entry effects the element in the $\left[k,l\right]$-th entry. Luckily for us, in 2009, Lewandowski, Kurowicka, and Joe <a href="https://www.sciencedirect.com/science/article/pii/S0047259X09000876" target="_blank">published</a> a method for generating random correlation matrices, aptly referred to as the LKJ distribution. Like other probablistic programming languages, TFP implements the LKJ distribution. It&rsquo;s a distribution that gets two numbers as inputs - $N$, the dimension of the correlation matrix, and $\eta$, a concentration parameter that controls how plausible are large correlations; Larger $\eta$ mean correlations are more concentrated around zero <sup class="footnote-ref" id="fnref:1"><a href="#fn:1">1</a></sup>.</p>

<pre><code class="language-python"># the necessary imports
import tensorflow as tf
import tensorflow_probability as tfp
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt 
import numpy as np
from matplotlib.patches import Ellipse
from tensorflow_probability import distributions as tfd
from tensorflow_probability import bijectors as tfb
tf.compat.v1.enable_eager_execution()

# for plotting
sns.set_palette(&quot;muted&quot;)

# for reproducibility
np.random.seed(324)
tf.random.set_random_seed(234)
</code></pre>

<p>Here&rsquo;s how samples from different LKJ distributions look like. We sample 500 correltion matrices with $\eta=1$ and 500 matrices with $\eta=50$:</p>

<pre><code class="language-python">plt.hist(tfd.LKJ(5,1).sample(500).numpy().flatten(), bins=np.linspace(-0.99,0.99), density=True, label=&quot;$\eta=1$&quot;)
plt.hist(tfd.LKJ(5,50).sample(500).numpy().flatten(), bins=np.linspace(-0.99,0.99), density=True, label=&quot;$\eta=50$&quot;)
plt.xlabel(&quot;Correlation values&quot;)
plt.title(&quot;500 5x5 correlation matrices&quot;)
plt.legend()
</code></pre>

<p><img src="1.png" alt="png" /></p>

<h2 id="problem-1-falling-off-the-manifold">Problem #1 - falling off the manifold</h2>

<p>So far so good - sampling correlation matrices seems straightforward. The problem starts when we want to use a Hamiltonian Monte Carlo (and we usually want to use Hamiltonian Monte Carlo) to sample from some larger model that contains an LKJ distribution. HMC allows us to generate samples from arbitrary joint distributions, not only from distributions for which we have explicit sampling methods. Here&rsquo;s a toy example to illustrate the problem. The model is simply a single LKJ distribution within a <code>JointDistributionSequential</code> object:</p>

<pre><code class="language-python">model =  tfd.JointDistributionSequential(
    [
        tfd.LKJ(2,2),
    ]
)
model.sample()
</code></pre>

<pre><code>[&lt;tf.Tensor: id=7897612, shape=(2, 2), dtype=float32, numpy=
 array([[ 1.        , -0.43337458],
        [-0.43337458,  1.        ]], dtype=float32)&gt;]
</code></pre>

<p><code>model.sample()</code> seems to work, so that&rsquo;s encouraging. However, when we try to &ldquo;naively&rdquo; use an HMC sampler to generate samples from the model, things go wrong. We add a small helper function to avoid rewriting all the kernels everytime; see the <a href="https://adamhaber.github.io/2019/07/08/A-Tutorial-on-Varying-Intercepts-Models-with-TensorFlow-Probability.html" target="_blank">previous post</a> for explanations about the different function calls here.</p>

<pre><code class="language-python">def sampleHMC(log_prob, inits, bijectors_list = None):
    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(
        target_log_prob_fn=log_prob,
        step_size=0.1,
        num_leapfrog_steps=3
    )
    if bijectors_list is not None:
        inner_kernel = tfp.mcmc.TransformedTransitionKernel(inner_kernel, bijectors_list)
        
    adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(
        inner_kernel=inner_kernel,
        num_adaptation_steps=400
    )
    return tfp.mcmc.sample_chain(
        num_results=500,
        current_state=inits,
        kernel=adaptive_kernel,
        num_burnin_steps=500,
        trace_fn=None
    )
</code></pre>

<pre><code class="language-python">lkj_samps = sampleHMC(
    log_prob=lambda lkj: model.log_prob([lkj]),
    inits=model.sample()
)[0]
lkj_samps[:3]  # we print the first 3 samples 
</code></pre>

<pre><code>&lt;tf.Tensor: id=8349304, shape=(3, 2, 2), dtype=float32, numpy=
array([[[ 28.062887,  81.77511 ],
        [-80.5103  , -68.75983 ]],

       [[ 65.458626,  72.01995 ],
        [-87.03769 , -59.71089 ]],

       [[104.11292 ,  85.66264 ],
        [-83.73789 , -60.2727  ]]], dtype=float32)&gt;
</code></pre>

<p>Here we can already see the problem; these aren&rsquo;t correlation matrices by any means. What&rsquo;s happening here is that HMC, which operates in an unconstrained space of real numbers, &ldquo;falls off&rdquo; the correlation matrices manifold. The solution for this is what&rsquo;s called a <strong>bijector</strong>. Without getting into the gory mathematical details <sup class="footnote-ref" id="fnref:2"><a href="#fn:2">2</a></sup>, a bijector can be thought of as a differentiable one-to-one mapping between the unconstrained space in which the HMC trajectories live, and the constrained manifold. HMC produces samples in the unconstrained space, and the appropriate bijector spits out a valid correlation matrix. For us, this bijector is <code>tfb.CorrelationCholesky()</code>. Note that we need to pass a list of bijectors to the <code>TransformedTransitionKernel</code> constructor; in this case, we&rsquo;re passing just a single bijector:</p>

<pre><code class="language-python">bij_lkj_samps = sampleHMC(
    log_prob=lambda lkj: model.log_prob([lkj]),
    inits=model.sample(),
    bijectors_list=[tfb.CorrelationCholesky()]
)[0]
bij_lkj_samps[:3]
</code></pre>

<pre><code>&lt;tf.Tensor: id=9873388, shape=(3, 2, 2), dtype=float32, numpy=
array([[[ 1.        ,  0.        ],
        [-0.43139905,  0.90216124]],

       [[ 1.        ,  0.        ],
        [-0.43139905,  0.90216124]],

       [[ 1.        ,  0.        ],
        [-0.43139905,  0.90216124]]], dtype=float32)&gt;
</code></pre>

<p>At first glance, these don&rsquo;t look like correlation matrices either; that&rsquo;s because they&rsquo;re the <em>Cholesky factors</em> of kosher correlation matrices.</p>

<h2 id="overthinking-box-cholesky-factors-3">Overthinking box - Cholesky factors <sup class="footnote-ref" id="fnref:3"><a href="#fn:3">3</a></sup></h2>

<p>Every correlation matrix $\Sigma$ can be decomposed as a product of a lower triangular matrix $L$ and its transpose $L^T$. More formally, a lower triangular matrix $L$ is the Cholesky factor of some correlation matrix $\Sigma$ if and only if its diagonal elements are strictly positive and each of its rows has unit norm.</p>

<p>Cholesky factors come up in many different places in statistics, machine learning, metric learning, computational linear algebra, etc. In the context of Monte Carlo simulations, Cholesky factors are used to generate correlated quantities (which we often want) from uncorrelated samples (which are easy to generate in the computer): If $z$ is a matrix of uncorrelated normally distributed numbers, and $L$ is the Cholesky factor of some correlation matrix $\Sigma$, then $Lz$ would have the correlation structure described by $\Sigma$.</p>

<p>Here&rsquo;s a simple demonstration. We generate 1000 samples from a bivariate guassian with zero mean, unit variance and no correlation:</p>

<pre><code class="language-python">z = tf.transpose(tfd.MultivariateNormalDiag(loc=[0,0], scale_diag=[1,1]).sample(1000))
</code></pre>

<p>We now define a correlation matrix between two variables with correlation -0.85. We compute its Cholesky factor, multiply it with the original (uncorrelated) data, and voila:</p>

<pre><code class="language-python">M = tf.constant([[1,-0.85],[-0.85,1]])
L = tf.cholesky(M)
Lz = L@z
</code></pre>

<pre><code class="language-python">plt.scatter(z[0], z[1], label=&quot;$z$&quot;)
plt.scatter(Lz[0], Lz[1], label=&quot;$Lz$&quot;)
plt.legend(loc='upper right')
</code></pre>

<p><img src="2.png" alt="png" /></p>

<p>We can see that the two components of $Lz$ are negatively correlated, as expected. More quantitatively, here&rsquo;s the correlation matrix for the cholesky-tranformed data:</p>

<pre><code class="language-python">tfp.stats.correlation(tf.transpose(Lz))
</code></pre>

<pre><code>&lt;tf.Tensor: id=9873636, shape=(2, 2), dtype=float32, numpy=
array([[ 0.9999995 , -0.8532509 ],
       [-0.8532509 ,  0.99999976]], dtype=float32)&gt;
</code></pre>

<h2 id="problem-2-the-wrong-log-prob">Problem #2 - the wrong log_prob</h2>

<p>So the bijector solves the constrained-unconstrained problem, and HMC can run smoothly. But things are trickier than that (and the sampler won&rsquo;t tell you that). The HMC sampler works with the log probability function of the model. If we have an LKJ distribution somewhere in our model, than for every sample, HMC computes the <code>log_prob</code> of the correlation matrix according to LKJ. But LKJ is a distribution over correlation matrices, not Cholesky factors of correlation matrices, which is the output of our bijector! So we end up computing the wrong <code>log_prob</code>, which means we&rsquo;re not sampling from the model we think we&rsquo;re sampling. So what can we do?</p>

<p>Solution number 1 is to make sure our cholesky-factors-of-correlation-matrices become correlation matrices before we compute their <code>log_prob</code> according to LKJ. To do so, we need two more bijectors: <code>tfb.CholeskyOuterProduct</code>, which maps $L$ to $LL^T$, and <code>tfb.Chain</code> which, surprisingly, chains (composes) the two bijectors:</p>

<pre><code class="language-python">chained_bij_samps = sampleHMC(
    lambda lkj: model.log_prob([lkj]), 
    model.sample(), 
    bijectors_list=[tfb.Chain([tfb.CholeskyOuterProduct(), tfb.CorrelationCholesky()])]
)[0]
chained_bij_samps[:3]
</code></pre>

<pre><code>&lt;tf.Tensor: id=12661134, shape=(3, 2, 2), dtype=float32, numpy=
array([[[ 1.        , -0.21354356],
        [-0.21354356,  1.        ]],

       [[ 1.        , -0.21354356],
        [-0.21354356,  1.        ]],

       [[ 1.        ,  0.01905983],
        [ 0.01905983,  1.0000001 ]]], dtype=float32)&gt;
</code></pre>

<p>This looks good. And this time it actually is - this is doing what we think it&rsquo;s doing. But this is cumbersome, and not very readable. Even worse, when we&rsquo;ll pass these correlations matrices to a multivariate gaussian (the usual case), it&rsquo;ll compute their cholesky factors anyway (check out the <a href="https://github.com/tensorflow/probability/blob/4dd589ba945db902d28dbb75dc0a795706814d45/tensorflow_probability/python/distributions/mvn_full_covariance.py#L189" target="_blank">source code</a>, as well as the depracation warning above it). So we end up sampling cholesky factors, tranforming them back to correlation matrices just to compute their cholesky factors again&hellip;</p>

<h2 id="enter-choleskylkj">Enter CholeskyLKJ</h2>

<p>Since <code>tfp-nightly-0.9.0.dev20190830</code> (a daily-built version that contains the newest changes that have yet to made it into the latest stable release), we have a better option - the <code>CholeskyLKJ</code> distribution. Unlike LKJ, this is a distribution over <em>cholesky factors</em> of correlation matrices - so no need to go back and forth, or to chain bijectors&hellip; It&rsquo;s faster, numerically stabler, and it is by the <a href="https://mc-stan.org/docs/2_19/functions-reference/cholesky-lkj-correlation-distribution.html" target="_blank">book</a>.</p>

<p>To use it, we just need a single <code>tfb.CorrelationCholesky()</code> bijector:</p>

<pre><code class="language-python">model =  tfd.JointDistributionSequential(
    [
        tfd.CholeskyLKJ(2,2),
    ]
)
</code></pre>

<pre><code class="language-python">cholesky_lkj_samps = sampleHMC(
    lambda lkj: model.log_prob([lkj]),
    model.sample(),
    bijectors_list=[tfb.CorrelationCholesky()]
)[0]
cholesky_lkj_samps[:3]
</code></pre>

<pre><code>&lt;tf.Tensor: id=14269238, shape=(3, 2, 2), dtype=float32, numpy=
array([[[ 1.        ,  0.        ],
        [ 0.20048861,  0.97969604]],

       [[ 1.        ,  0.        ],
        [-0.19122852,  0.9815455 ]],

       [[ 1.        ,  0.        ],
        [ 0.18798688,  0.9821716 ]]], dtype=float32)&gt;
</code></pre>

<h1 id="a-simple-use-case">A simple use case</h1>

<p>We&rsquo;ve covered the technicalities of sampling correlation matrices (and their Cholesky factors) with TFP. To get a more complete picture of how these are actually used, let&rsquo;s see an example. We&rsquo;re sticking with McElreath and Statistical Rethinking; this time we&rsquo;re reproducing the café waiting times example.</p>

<h2 id="fake-data">Fake data</h2>

<p>Unlike the tadpoles example, this time we&rsquo;re going to model fake data (aka synthetic data). This may sound strange, but it&rsquo;s actually a <em>very</em> useful skill, and it&rsquo;s considered by many to be pretty much the first step in a Bayesian data analysis workflow (see <a href="https://khakieconomics.github.io/2017/04/30/An-easy-way-to-simulate-fake-data-in-stan.html" target="_blank">here</a>). The reason is that unlike in a &ldquo;real data analysis&rdquo;, when you&rsquo;re generating fake data, you <em>know</em> the true underlying data generating process; making sure you can recover its parameters is a very important sanity check. It also helps in verifying the model is correctly specified and that the MCMC sampler does what you think it does, which is good.</p>

<p>The data we&rsquo;re generating describes the waiting times in 20 different cafés. Each café has a different average waiting times in the morning and in the afternoon. The average morning waiting time is the intercept, and the difference between afternoon and morning average waiting times is the slope. The intercepts and slopes for each of the 20 cafés are sampled from a (surprise surprise) correlated bivariate Gaussian distribution.</p>

<pre><code class="language-python">##### Inputs needed to generate the covariance matrix between intercepts and slopes #####


a = 3.5  # average morning wait time
b = -1 # average difference afternoon wait time
sigma_a = 1 # standard deviation in the (café-specific) intercepts
sigma_b = 0.5 # standard deviation in the (café-specific) slopes
rho = -0.7 # correlation between intercepts and slopes

mu = [a,b] # the mean of our gaussian distribution
sigmas = [sigma_a,sigma_b] # vector of standard deviations
corr_matrix = np.array([[1,rho], [rho,1]]) # correlation matrix
cov_matrix = np.diag(sigmas)@corr_matrix@np.diag(sigmas)  # the covariance matrix of our gaussian distribution 
</code></pre>

<p>After setting the true parameters, we&rsquo;re generating 20 samples of cafés:</p>

<pre><code class="language-python">n_cafés = 20 # 20 cafés overall

café_params = np.random.multivariate_normal(mu ,cov_matrix,size=n_cafés) 
café_intercept = café_params[:, 0] # intercepts are in the first column
café_slopes = café_params[:, 1] # slopes are in the second
</code></pre>

<p>And compute the actual per-café morning and afternoon waiting times, in 10 different visits. Below is a sample of 10 rows from our dataframe (which has 200 data points overall - 10 visits in 20 cafés):</p>

<pre><code class="language-python">n_visits = 10 # 10 visits per café

afternoon = np.tile([0,1], n_visits * n_cafés//2) # alternate values for mornings and afternoons in the data frame
café_id = np.repeat(np.arange(n_cafés),n_visits) # data for each café are consecutive rows in the data frame

mu = café_intercept[café_id] + café_slopes[café_id] * afternoon # the regression equation for the mean waiting time
sigma = 0.5 # standard deviation of waiting time within cafés
wait = np.random.normal(mu, sigma, n_visits * n_cafés) # generate instances of waiting times
df = pd.DataFrame(dict(café = café_id, afternoon = afternoon, wait = wait))
print(df.sample(10).to_string(index=False))
</code></pre>

<pre><code> café  afternoon      wait
    8          1  2.175858
    9          0  2.364313
    9          1  1.744504
   14          0  3.716937
    3          1  1.419163
    0          1  1.959044
    5          0  1.045913
    4          0  1.083699
   17          1  2.796278
   15          1  3.430852
</code></pre>

<h2 id="the-model">The model</h2>

<p>We specify in math (and latex) the model described above:</p>

<p>$$\begin{align}
W_i &amp; \sim \text{Normal}(\alpha_{café[i]}+\beta_{café[i]}\cdot \text{AFTERNOON}_i,\sigma) \\<br />
\binom{\alpha_{café}}{\beta_{café}} &amp; \sim \text{MVNormal}\left(\binom{\alpha}{\beta},\mathbb{S}\right) \\<br />
\mathbb{S} &amp; = \left(\begin{smallmatrix} \sigma_\alpha &amp; 0 \\\ 0 &amp; \sigma_\beta \end{smallmatrix}\right) \cdot LL^T \cdot \left(\begin{smallmatrix} \sigma_\alpha &amp; 0 \\\ 0 &amp; \sigma_\beta \end{smallmatrix}\right)  \\<br />
\alpha &amp; \sim \text{Normal}(5,2) \\<br />
\beta &amp; \sim \text{Normal}(-1,0.5) \\<br />
\sigma_{\alpha},\sigma_{\beta} &amp; \sim \text{Exp}(1) \\<br />
\sigma &amp; \sim \text{Exp}(1) \\<br />
L &amp; \sim \text{CholeskyLKJ}(2,2) \\<br />
\end{align}$$</p>

<pre><code class="language-python">model = tfd.JointDistributionSequential(
    [
        tfd.CholeskyLKJ(2,2),  # rho, the prior for the correlation matrix between intercepts and slopes
        tfd.Sample(tfd.Exponential(rate = 1),sample_shape = 1), # sigma, prior std for the waiting time
        tfd.Sample(tfd.Exponential(rate = 1),sample_shape = 2), # sigma_café, prior of stds for intercepts and slopes (vector of 2)
        tfd.Sample(tfd.Normal(loc = -1, scale = 0.5), sample_shape = 1),   # b, the prior mean for the slopes
        tfd.Sample(tfd.Normal(loc = 5, scale = 2), sample_shape = 1),   # a, the prior mean for the intercepts
        lambda a,b,sigma_café,sigma,chol_rho : tfd.Sample( # per-café intercepts and slopes
            tfd.MultivariateNormalTriL(
                loc = tf.concat([a,b],axis=-1),
                scale_tril = tf.linalg.LinearOperatorDiag(sigma_café).matmul(chol_rho)
            ),
            sample_shape=n_cafés
        ),
        lambda mvn, a, b, sigma_café, sigma : tfd.Independent(  #per-café waiting times
            tfd.Normal(
                loc = tf.gather(mvn[:,:,0],café_id,axis=-1) + tf.gather(mvn[:,:,1],café_id,axis=-1)*afternoon,
                scale = sigma
            ),
            reinterpreted_batch_ndims=1
        )
    ]
)
</code></pre>

<p>Couple of non-trivial things in the model above:</p>

<ul>
<li><code>MultivariateNormalTriL</code>: we&rsquo;ve mentioned that a covariance matrix can be specified as $\Lambda L L^T\Lambda$ where $\Lambda$ is a diagonal matrix of standard deviations and $L$ is the cholesky factor of the correlation matrix. <code>MultivariateNormalTriL</code> is a parametrization of a multivariate normal distribution whose covariance matrix is specificied using the lower triangular matrix $\Lambda L$.</li>
<li><code>LinearOperatorDiag</code>: this turns a <code>sigma-café</code> vector of length 2 to a 2x2 diagonal matrix; very similar to <code>tf.diag</code>, but handles all the batching semantics for us.</li>
<li><code>tf.gather</code>: this takes each intercept (in the case of <code>mvn[:,:,0]</code>) and slope (in the case of <code>mvn[:,:,1]</code> and tiles it 10 times, so overall we get a loc vector of size 200, with which we generate 200 different waiting times, 10 per café.</li>
</ul>

<p>We now declare the <code>target_log_prob</code> function for the HMC kernel, and initial values for 4 different chains. Like before, we throw away the last sample (predicted waiting times); we want to plug the waiting times from the data into the likelihood, instead.</p>

<pre><code class="language-python">n_chains = 4
log_prob_fn = lambda rho, sigma, sigma_café, b, a, mvn : model.log_prob([rho, sigma, sigma_café, b, a, mvn ,wait])
init_rho, init_sigma, init_sigma_café, init_b, init_a, init_mvn, _ = model.sample(n_chains)
</code></pre>

<p>These initial values are used to specify the shape of the initial values we actually pass, specified below:</p>

<pre><code class="language-python">init_rho = tf.stack([tf.eye(2) for _ in range(n_chains)])
init_sigma = tf.ones_like(init_sigma)
init_sigma_café = tf.ones_like(init_sigma_café)
init_b = tf.zeros_like(init_b)
init_a = tf.zeros_like(init_a)
init_mvn = tf.zeros_like(init_mvn)
</code></pre>

<p>We define the list of bijectors. Note that since standard deviations are non-negative, their support is constrained, and we need a bijector here, as well. The appropriate bijector in this case is <code>tfb.Exp</code>. Once we specificed a bijectors list, we need to match a bijector for any distribution in our <code>JointDistributionSequential</code> object; since the support of <code>a</code>, <code>b</code> and <code>mvn</code> is unconstrained, we simply use an identity transformation:</p>

<pre><code class="language-python">bijectors_list = [
    tfb.CorrelationCholesky(),
    tfb.Exp(),
    tfb.Exp(),
    tfb.Identity(),
    tfb.Identity(),
    tfb.Identity(),
]
</code></pre>

<pre><code class="language-python">states = sampleHMC(log_prob_fn,
                [init_rho, init_sigma, init_sigma_café, init_b, init_a, init_mvn],
                bijectors_list)
</code></pre>

<pre><code class="language-python">[s.shape for s in states]
</code></pre>

<pre><code>[TensorShape([Dimension(500), Dimension(4), Dimension(2), Dimension(2)]),
 TensorShape([Dimension(500), Dimension(4), Dimension(1)]),
 TensorShape([Dimension(500), Dimension(4), Dimension(2)]),
 TensorShape([Dimension(500), Dimension(4), Dimension(1)]),
 TensorShape([Dimension(500), Dimension(4), Dimension(1)]),
 TensorShape([Dimension(500), Dimension(4), Dimension(20), Dimension(2)])]
</code></pre>

<p>Shapes look alright. To see the posterior distribution of covariance values, we move back from Cholesky factors to correlation matrices, and multiply by the inferred sigmas (the zeroth axis is the number of samples, first is the number of the chain, so we transpose the second and third axes):</p>

<pre><code class="language-python">rhos = states[0]@tf.transpose(states[0],[0,1,3,2])
</code></pre>

<p>Same as above, we create diagonal matrices from our sampled <code>sigma_alpha</code>, <code>sigma_beta</code> values:</p>

<pre><code class="language-python">sigmas = states[2]
diag_sigmas = tf.linalg.LinearOperatorDiag(sigmas)
</code></pre>

<pre><code class="language-python">inferred_covs = tf.matmul(diag_sigmas.matmul(rhos),diag_sigmas)
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(9,4))
for (row_idx,col_idx), title in zip([(0,0),(0,1),(1,1)],[&quot;$\sigma_{\\alpha}$&quot;,&quot;Covariance&quot;,&quot;$\sigma_{\\beta}$&quot;]):
    plt.subplot(131+row_idx+col_idx)
    sns.distplot(inferred_covs[:,:,row_idx,col_idx].numpy().flatten(), label = &quot;Posterior&quot;)
    plt.axvline(cov_matrix[row_idx,col_idx],c='k',ls='--',label=&quot;True value&quot;)
    plt.legend()
    plt.title(title)
plt.tight_layout()
</code></pre>

<p><img src="3.png" alt="png" /></p>

<p>We can also compare empirical waiting times with sampled waiting times:</p>

<pre><code class="language-python">morning_wait_emp = df.query('afternoon == 0').groupby('café').wait.mean()
afternoon_wait_emp = df.query('afternoon == 1').groupby('café').wait.mean()
</code></pre>

<pre><code class="language-python">morning_wait_pred = tf.reduce_mean(states[-1][:,:,:,0], axis=(0,1))
afternoon_wait_pred = tf.reduce_mean(states[-1][:,:,:,1], axis=(0,1)) + morning_wait_pred
</code></pre>

<p>And we get the shrinkage that decorates Statistical Rethinking&rsquo;s front cover:</p>

<pre><code class="language-python">plt.figure(figsize=(9,6))
ax = plt.subplot(111)
vals, vecs = np.linalg.eigh(np.cov(morning_wait_emp, afternoon_wait_emp))
theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))
w, h = 2 * np.sqrt(vals)
for contour_line in range(1,5):
    ell = Ellipse(xy=(np.mean(morning_wait_emp), np.mean(afternoon_wait_emp)),
                  width=w*contour_line, height=h*contour_line,
                  angle=theta, color='black')
    ell.set_facecolor('none')
    ax.add_artist(ell)
plt.scatter(morning_wait_emp,afternoon_wait_emp,label=&quot;Empirical&quot;)
plt.scatter(morning_wait_pred,afternoon_wait_pred, label=&quot;MCMC&quot;)
plt.scatter(morning_wait_emp.mean(),afternoon_wait_emp.mean(),marker='+',c='r',s=100, label=&quot;Grand Mean&quot;)
plt.legend()
for a,b,c,d in zip(morning_wait_emp, afternoon_wait_emp,  morning_wait_pred, afternoon_wait_pred):
    plt.arrow(a,b,0.7*(c.numpy()-a),0.7*(d.numpy()-b), head_width=0.05, alpha=0.3)
plt.xlabel(&quot;Morning wait&quot;)
plt.ylabel(&quot;Afternoon wait&quot;)
</code></pre>

<p><img src="4.png" alt="png" /></p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">Formally, the distribution is defined as: $\text{LKJ}\left(\Sigma\vert\eta\right)\propto\det\left(\Sigma\right)^{\left(\eta-1\right)}$. Intuitively, the correlation matrix defines an ellipsoid in $N$ dimensions, and its determinant is the volume of the ellipsoid. So, higher correlations -&gt; tighter ellipsoid -&gt; smaller volume -&gt; smaller determinant -&gt; more likely for small $\eta$ and less likely for large $\eta$.
 <a class="footnote-return" href="#fnref:1"><sup>^</sup></a></li>
<li id="fn:2">Sigrid Keydana did an excellent job explaining TFP bijectors, and specifcally the intuition behind the jacobian correction, in <a href="https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows/" target="_blank">this</a> post.
 <a class="footnote-return" href="#fnref:2"><sup>^</sup></a></li>
<li id="fn:3">Overthinking boxes are specific (usually mathematical) dive-ins in Statistical Rethinking.
 <a class="footnote-return" href="#fnref:3"><sup>^</sup></a></li>
</ol>
</div>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/tfp/">TFP</a>
  
  <a class="badge badge-light" href="/tags/multilevel-models/">Multilevel Models</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://adamhaber.github.io/post/varying-slopes/&amp;text=Varying%20Slopes%20Models%20and%20the%20CholeskyLKJ%20distribution%20in%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://adamhaber.github.io/post/varying-slopes/&amp;t=Varying%20Slopes%20Models%20and%20the%20CholeskyLKJ%20distribution%20in%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Varying%20Slopes%20Models%20and%20the%20CholeskyLKJ%20distribution%20in%20TensorFlow%20Probability&amp;body=https://adamhaber.github.io/post/varying-slopes/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://adamhaber.github.io/post/varying-slopes/&amp;title=Varying%20Slopes%20Models%20and%20the%20CholeskyLKJ%20distribution%20in%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Varying%20Slopes%20Models%20and%20the%20CholeskyLKJ%20distribution%20in%20TensorFlow%20Probability%20https://adamhaber.github.io/post/varying-slopes/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://adamhaber.github.io/post/varying-slopes/&amp;title=Varying%20Slopes%20Models%20and%20the%20CholeskyLKJ%20distribution%20in%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hud138a96a47a9e2657537fb19ee618cb6_16021_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://adamhaber.github.io/">Adam Haber</a></h5>
      <h6 class="card-subtitle">Computational Neuroscience PhD Student</h6>
      <p class="card-text">Interested in probabilistic programming, computational statistics, statistical physics and programming languages.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#mailto:adamhaber@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/_adam_haber" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/adamhaber" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>






<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>







<div class="article-widget content-widget-hr">
  <h3>Related</h3>
  <ul>
    
    <li><a href="/post/varying-intercepts/">A Tutorial on Varying Intercepts Models with TensorFlow Probability</a></li>
    
  </ul>
</div>



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.96cf4c3dc37ea60dbbd03c13a455f1f7.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
