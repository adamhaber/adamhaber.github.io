<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Adam Haber">

  
  
  
    
  
  <meta name="description" content="Intro This post is about building varying intercepts models using TensorFlow Probability (&ldquo;TFP&rdquo;). It&rsquo;s basically my attempt to translate Sigrid Keydana&rsquo;s wonderful blog post from R to Python. I&rsquo;m doing this for a couple of reasons: First, I&rsquo;ve played with TFP before, was quite impressed by its performance and flexibility, and wanted to learn more about it; Second, I wanted to start blogging, and this seemed like an easy start; Last, TFP is rather new, and there aren&rsquo;t a whole lot of resources and tutorials about it - so this might even prove useful to someone, someday.">

  
  <link rel="alternate" hreflang="en-us" href="https://adamhaber.github.io/post/varying-intercepts/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-143367176-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-143367176-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://adamhaber.github.io/post/varying-intercepts/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@_adam_haber">
  <meta property="twitter:creator" content="@_adam_haber">
  
  <meta property="og:site_name" content="Adam Haber">
  <meta property="og:url" content="https://adamhaber.github.io/post/varying-intercepts/">
  <meta property="og:title" content="A Tutorial on Varying Intercepts Models with TensorFlow Probability | Adam Haber">
  <meta property="og:description" content="Intro This post is about building varying intercepts models using TensorFlow Probability (&ldquo;TFP&rdquo;). It&rsquo;s basically my attempt to translate Sigrid Keydana&rsquo;s wonderful blog post from R to Python. I&rsquo;m doing this for a couple of reasons: First, I&rsquo;ve played with TFP before, was quite impressed by its performance and flexibility, and wanted to learn more about it; Second, I wanted to start blogging, and this seemed like an easy start; Last, TFP is rather new, and there aren&rsquo;t a whole lot of resources and tutorials about it - so this might even prove useful to someone, someday."><meta property="og:image" content="https://adamhaber.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://adamhaber.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-07-08T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-07-08T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://adamhaber.github.io/post/varying-intercepts/"
  },
  "headline": "A Tutorial on Varying Intercepts Models with TensorFlow Probability",
  
  "datePublished": "2019-07-08T00:00:00Z",
  "dateModified": "2019-07-08T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Adam Haber"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Adam Haber",
    "logo": {
      "@type": "ImageObject",
      "url": "https://adamhaber.github.io/img/icon-512.png"
    }
  },
  "description": "Intro This post is about building varying intercepts models using TensorFlow Probability (\u0026ldquo;TFP\u0026rdquo;). It\u0026rsquo;s basically my attempt to translate Sigrid Keydana\u0026rsquo;s wonderful blog post from R to Python. I\u0026rsquo;m doing this for a couple of reasons: First, I\u0026rsquo;ve played with TFP before, was quite impressed by its performance and flexibility, and wanted to learn more about it; Second, I wanted to start blogging, and this seemed like an easy start; Last, TFP is rather new, and there aren\u0026rsquo;t a whole lot of resources and tutorials about it - so this might even prove useful to someone, someday."
}
</script>

  

  


  


  





  <title>A Tutorial on Varying Intercepts Models with TensorFlow Probability | Adam Haber</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    
    
      <a class="navbar-brand" href="/">Adam Haber</a>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  



















<div class="article-container pt-3">
  <h1>A Tutorial on Varying Intercepts Models with TensorFlow Probability</h1>

  

  
  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jul 8, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  

  
  

</div>

  














  
</div>



  <div class="article-container">

    <div class="article-style">
      

<h1 id="intro">Intro</h1>

<p>This post is about building varying intercepts models using TensorFlow Probability (&ldquo;TFP&rdquo;). It&rsquo;s basically my attempt to translate Sigrid Keydana&rsquo;s wonderful <a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/" target="_blank">blog post</a> from R to Python. I&rsquo;m doing this for a couple of reasons: First, I&rsquo;ve played with TFP before, was quite impressed by its performance and flexibility, and wanted to learn more about it; Second, I wanted to start blogging, and this seemed like an easy start; Last, TFP is rather new, and there aren&rsquo;t a whole lot of resources and tutorials about it - so this might even prove useful to someone, someday.</p>

<p>Sigrid dedicated her post to <a href="https://twitter.com/rlmcelreath" target="_blank">Richard McElreath</a> and his book; I&rsquo;d like to join her on that. I was looking for a good introduction to Bayesian stats for quite some time. BDA3 was too technical for me at that point, Kruschke&rsquo;s was excellent but didn&rsquo;t really dive into the more sophisticated topics I wanted to learn. Statistical Rethinking was spot on - interesting, fun to read, and super helpful. It&rsquo;s very code-oriented, and has already been re-written in pure stan, brms, pymc3, julia and probably many others.</p>

<p>Stats-wise, this post is going to be about varying intercepts models, which are perhaps the simplest kind of a multilevel model. The main idea behind them - called partial pooling - is simple and beautiful, but here I want to focus on the code, not the stats; for a nice introductory demo, check out <a href="http://mfviz.com/hierarchical-models/" target="_blank">this</a> beautiful visualization, or <a href="http://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/" target="_blank">this</a> one. Better yet, get a copy of Statistical Rethinking and read the original. :-)</p>

<h1 id="the-data">The data</h1>

<p>We&rsquo;re given data about 48 different tanks containing tadpoles (pre-frogs). Each tank has a <code>density</code> (the initial number of tadpoles in it), a categorical feature <code>pred</code> (whether the tank contained a predator or not), a categorical feature <code>size</code> (big tank or small tank), the number of surviving tadpoles <code>surv</code> and the proportion of surviving tadpoles <code>propsurv</code> (which is simply <code>surv</code>/<code>density</code>). The original data came with the book&rsquo;s R package; Luckily, it&rsquo;s hosted in Osvaldo Martin&rsquo;s <a href="https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3" target="_blank">repo</a>:</p>

<pre><code class="language-python">import pandas as pd
df = pd.read_csv(&quot;https://raw.githubusercontent.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3/master/Data/reedfrogs.csv&quot;)
df.head()
</code></pre>

<table>
<thead>
<tr>
<th align="right">Tank</th>
<th align="right">density</th>
<th align="right">pred</th>
<th align="right">size</th>
<th align="right">surv</th>
<th>propsurv</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">0</td>
<td align="right">10</td>
<td align="right">no</td>
<td align="right">big</td>
<td align="right">9</td>
<td>0.9</td>
</tr>

<tr>
<td align="right">1</td>
<td align="right">10</td>
<td align="right">no</td>
<td align="right">big</td>
<td align="right">10</td>
<td>1.0</td>
</tr>

<tr>
<td align="right">2</td>
<td align="right">10</td>
<td align="right">no</td>
<td align="right">big</td>
<td align="right">7</td>
<td>0.7</td>
</tr>

<tr>
<td align="right">3</td>
<td align="right">10</td>
<td align="right">no</td>
<td align="right">big</td>
<td align="right">10</td>
<td>1.0</td>
</tr>

<tr>
<td align="right">4</td>
<td align="right">10</td>
<td align="right">no</td>
<td align="right">small</td>
<td align="right">9</td>
<td>0.9</td>
</tr>
</tbody>
</table>

<p>Tank densities are either 10, 25 or 35.</p>

<h1 id="the-model">The model</h1>

<p>Our goal is to compute the probability of survival in each of the tanks. <code>propsurv</code> is one way to do this, which is straightforward and intuitive - simply compute the per-tank ratio of surviving tadpoles. But this doesn&rsquo;t make much sense, especially if you consider the small sample sizes - if I&rsquo;d give you a tank with <code>density=1</code>, would you feel comfortable with saying that the probability of survival is either 0% (<code>surv=0</code>) or 100% (<code>surv=1</code>)? Probably not.</p>

<p>A different approach would be to ignore between-tanks variations, and assume all tanks have exactly the same probability of survival. Our best estimate is then the ratio of all the surviving tadpoles (in all tanks, combined) - or <code>sum(surv)/sum(density)</code>.</p>

<p>A varying intercept model is somewhat in between - it assumes each tank has its own probability of survival, but that all these probabilities are coming from some distribution over &ldquo;probabilities of survival&rdquo;. This is how it looks like:</p>

<p>$$
\begin{align}
\bar{\alpha} &amp; \sim \text{Normal}(0,1.5) \\<br />
\sigma &amp; \sim \text{Exponential}(1) \\<br />
\text{logit}\left(p_i\right) &amp; \sim \text{Normal}\left(\bar{\alpha},\sigma\right) \\<br />
s_i &amp; \sim \text{Binomial}(n_i,p_i) \\<br />
\end{align}
$$</p>

<p>What&rsquo;s all this? In short - we assume the logits of the survival probabilities are sampled from some normal distribution, whose parameters (often called &ldquo;hyperparameters&rdquo;) we&rsquo;re trying to infer. <code>a_bar</code> is the mean of this normal distribution, and we put a generic weakly informative prior on it - normal(0,1.5). <code>sigma</code> is the standard deviation of this normal distribution, and we put an exponential prior on it. After sampling these two numbers, we plug them into the logits distribution, sample 48 different logit values, transform them to probabilities and sample 48 survival predictions from the binomial distributions (one per tank).  Now to the code itself. We begin with the necessary imports:</p>

<pre><code class="language-python">import tensorflow as tf
import tensorflow_probability as tfp
import seaborn as sns
import matplotlib.pyplot as plt 
import numpy as np
tfd = tfp.distributions
</code></pre>

<p>For ease-of-use, we&rsquo;re using TensorFlow in Eager mode, which allows a more interactive and iterative workflow.</p>

<pre><code class="language-python">tf.compat.v1.enable_eager_execution()
</code></pre>

<h2 id="some-tfp-s-pre-requisites">Some TFP&rsquo;s pre-requisites</h2>

<p>Before we start implementing the model itself, we need to cover some of the basic terminology around a <code>TensorFlow Distribution</code>. For the purposes of this introductory post, you can think of a distribution as an object with the following two methods:</p>

<ul>
<li><code>sample()</code></li>
<li><code>log_prob()</code></li>
</ul>

<p>Both are pretty straightforward - <code>sample()</code> allows you to generate samples from a given distribution; <code>log_prob()</code> allows you to calculate the log-probability of a given value(s). There are other methods, of course, but these are the important ones for us.
There are two more attributes we need to mention:</p>

<ul>
<li><code>event_shape</code></li>
<li><code>batch_shape</code></li>
</ul>

<p>These were, at least for me, quite confusing (despite their pretty good <a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb" target="_blank">docs</a>). <code>event_shape</code> is the simpler of the two - if I have some joint probability distribution over N random variables, its <code>event_shape</code> is N. For example, a bivariate gaussian would have an event shape of 2.</p>

<p><code>batch_shape</code> is trickier: TFP allows you to create a single Distribution object, which actually contains multiple, independent distributions. For example, <code>tfd.Bernoulli(probs=[.3, .5, .7])</code> is a Distribution object composed of 3 different Bernoulli random variables (RVs) with probabilities of success .3, .5 and .7. The number of the independent distributions contained in this single object is its <code>batch_shape</code>. Why do this? My best guess is that it gives TFP the ability to make use of the underlying TF infrastructure, in which batching (and broadcasting along a batch dimension) is a fundamental operation. We&rsquo;ll get back to this in the code below.</p>

<p>Now we&rsquo;ll go ahead and define the model itself using TFP&rsquo;s <code>JointDistributionSequential</code> API:</p>

<pre><code class="language-python">m = tfd.JointDistributionSequential(
    [
        tfd.Normal(loc=0, scale=1.5),
        tfd.Exponential(rate=1.),
        lambda sigma, a_bar: tfd.Sample(tfd.Normal(loc=a_bar, scale=sigma),sample_shape=[df.shape[0]]),
        lambda l: tfd.Independent(tfd.Binomial(total_count=df.density.astype('float32'), logits=l),
                                  reinterpreted_batch_ndims=1)
    ]
)
</code></pre>

<p>The main workhorse here is <code>tfd.JointDistributionSequential</code>, which is very similar to <code>Sequential</code> in Keras or PyTorch. It&rsquo;s an object composed of list of Distribution-making functions (<code>tfd.Distribution</code>s or Python callables that return a <code>tfd.Distribution</code>). The idea of sequentially stacking distributions, and adding the dependencies between them (the fact that the values sampled from <code>tfd.Normal</code> and <code>tfd.Exponential</code> are &lsquo;fed&rsquo; into the 3rd distribution as its mean and standard deviation) is simple and intuitive, and fits nicely in the hierarchical modeling workflow; the code above is basically a 1-to-1 translation of the model specification.</p>

<p>The tricky parts here are TFP&rsquo;s <code>Sample</code> and <code>Independent</code>. What are these, then?</p>

<ul>
<li><code>Sample</code> - The third function receives the hyper-parameters <code>sigma</code> and <code>a_bar</code>, and should return one number per tank, drawn from a <code>normal(a_bar,sigma)</code>. <code>tfd.Sample</code> allows us to draw samples from the product distribution of all these 48 Gaussians; each sample from <code>Sample</code> is a vector of 48 (uncorrelated) numbers, all with the same mean <code>a_bar</code> and standard deviation <code>sigma</code>.</li>
<li><code>Independent</code> - the third distribution returns a vector of 48 numbers. If we simply write <code>tfd.Binomial(total_count=df.density.astype('float32'), logits=l)</code>, we&rsquo;ll get a distribution with a <code>batch_shape</code> of 48 and an <code>event_shape</code> (), representing a scalar output. Wrapping this with <code>tfd.Independent</code> transforms this output to be of <code>batch_shape</code> () and <code>event_shape</code> 48, representing a vector output, like we want it to be.</li>
</ul>

<p>Another possibly-confusing issue here is the order of the parameters in the lambda expressions. The first parameter is the output of the previous distribution in the list, the second parameter is the output of the previous-previous distribution, etc&hellip; This is why they third distribution gets <code>sigma</code> before <code>a_bar</code> despite the fact <code>sigma</code> is defined <em>after</em> <code>a_bar</code>.</p>

<p>I found this API somewhat different than the &ldquo;natural&rdquo; way to think about the problem; however, if this ends up with  superior performance, it&rsquo;s probably worth the learning curve for a wide enough range of problems.</p>

<h1 id="sampling-from-a-model">Sampling from a model</h1>

<p>The model&rsquo;s <code>sample()</code> method gets a <code>sample_shape</code> argument which determines the shape of the generated sample. This, in turn, will be used to tell the MCMC sampler how many chains to run in parallel.</p>

<pre><code class="language-python">n_chains = 4

initial_a, initial_s, initial_logits, init_surv = m.sample(n_chains)
</code></pre>

<p>Since we&rsquo;ve asked for 4 chains, <code>m.sample()</code> returns 4 samples from the <code>a_bar</code> hyperprior and 4 samples from the <code>sigma</code> hyperprior; these, in turn, generate 4 new normal distributions, from which we sample 4x48 logit values. These values are then &ldquo;pushed forward&rdquo;, generating 4x48 samples from the binomial survival distributions. These survival predictions can (and probably should) be used to perform prior predictive checks, but we don&rsquo;t need them to define the sampler, itself.</p>

<pre><code class="language-python">initial_a.shape, initial_s.shape, initial_logits.shape, init_surv.shape
</code></pre>

<pre><code>(TensorShape([Dimension(4)]),
 TensorShape([Dimension(4)]),
 TensorShape([Dimension(4), Dimension(48)]),
 TensorShape([Dimension(4), Dimension(48)]))
</code></pre>

<p>Now we create the sampler object. This step is composed of 3 different TFP objects. The first is the Hamiltonian Monte Carlo transition kernel, which uses the model&rsquo;s <code>.log_prob()</code> function:</p>

<pre><code class="language-python">inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(
    target_log_prob_fn = lambda x,y,z : m.log_prob([x,y,z,df.surv.astype('float32')]),
    step_size=0.1,
    num_leapfrog_steps=3
)
</code></pre>

<p>Note that we&rsquo;re not using the model&rsquo;s <code>.log_prob()</code> <em>as is</em>; instead, we make sure that the log-probability is always computed with respect to the actual, observed survival data. This is the purpose of the <code>lambda</code> function above. For the other two required parameters, I&rsquo;m using the ones from Sigrid&rsquo;s post.</p>

<p>The second part is the <code>SimpleStepSizeAdaptation</code> object, which takes the kernel defined above and returns a new kernel with dynamic step size adaptation:</p>

<pre><code class="language-python">kernel = tfp.mcmc.SimpleStepSizeAdaptation(
    inner_kernel=inner_kernel,
    target_accept_prob = 0.8,
    num_adaptation_steps = 500
)
</code></pre>

<p>Lastly, the sampling function. This object takes as input the initial states (and through them, number of chains to run), number of burnin steps, number of steps to run after burnin, a kernel (our augmented HMC kernel), and a trace function, which determines what kind of intermediate results we want to save. After sampling ends (this can take a while, depending on the complexity of your model), the function returns the samples (and traced results). Here I&rsquo;ve decided not to save intermediate results, at all; the simple diagnostics I&rsquo;m interested in can be computed from the samples themselves.</p>

<pre><code class="language-python">a_bars, sigmas, logits = tfp.mcmc.sample_chain(
    current_state=[
        tf.zeros_like(initial_a), 
        tf.ones_like(initial_s),
        initial_logits
    ],
    num_results=500,
    num_burnin_steps=500,
    kernel=kernel,
    trace_fn=None
)
</code></pre>

<p>Let&rsquo;s have a look at the output shapes:</p>

<pre><code class="language-python">a_bars.shape, sigmas.shape, logits.shape
</code></pre>

<pre><code>(TensorShape([Dimension(500), Dimension(4)]),
 TensorShape([Dimension(500), Dimension(4)]),
 TensorShape([Dimension(500), Dimension(4), Dimension(48)]))
</code></pre>

<p>The sampler returned 500 samples per chain per parameter - exactly what we want.</p>

<p>TFP provides standard MCMC diagnostics, such as effective sample size per logit parameter (we average over chains):</p>

<pre><code class="language-python">tf.reduce_mean(tfp.mcmc.effective_sample_size(logits),axis=0)
</code></pre>

<pre><code>&lt;tf.Tensor: id=1643926, shape=(48,), dtype=float32, numpy=
array([ 39.01448 ,  27.656044,  64.033554,  23.641933,  38.67304 ,
        43.81818 ,  27.42485 ,  56.60047 ,  86.46597 ,  53.400955,
        62.463234,  68.98154 ,  71.84833 ,  85.98772 ,  53.90014 ,
        45.368874,  57.142807,  64.70456 ,  80.501144,  30.96955 ,
        51.123882,  90.971016,  83.67827 , 133.95776 , 147.53087 ,
       237.56706 , 124.73306 , 213.96054 , 255.63586 , 127.86496 ,
       169.16728 , 222.58665 ,  57.26799 ,  62.313004,  59.934887,
       140.44281 , 126.62906 ,  44.229973,  80.57881 , 100.344055,
       102.71631 , 307.0775  , 298.0421  , 298.77765 , 275.2672  ,
       241.8357  , 134.39154 , 334.7177  ], dtype=float32)&gt;
</code></pre>

<p>And R-hat values:</p>

<pre><code class="language-python">tfp.mcmc.potential_scale_reduction(logits)
</code></pre>

<pre><code>&lt;tf.Tensor: id=1643986, shape=(48,), dtype=float32, numpy=
array([1.0393666, 1.0419586, 1.014595 , 1.0326122, 1.0066991, 1.0365033,
       1.1032237, 1.0193528, 1.0026469, 1.03286  , 1.0212902, 1.0046616,
       1.0046593, 1.0190336, 1.0491707, 1.0185002, 1.0236404, 1.0240865,
       1.0135043, 1.0091226, 1.0240618, 1.0212263, 1.0040884, 1.0057993,
       1.0115193, 1.0074626, 1.0053723, 1.0013644, 1.0038955, 1.0128344,
       1.0199273, 1.0040274, 1.1276014, 1.0033313, 1.0127679, 1.0017091,
       1.0118763, 1.0570774, 1.04308  , 1.0189458, 1.0144566, 1.0009695,
       1.0063009, 1.0008804, 1.0011569, 1.0057343, 1.0079254, 1.0071955],
      dtype=float32)&gt;
</code></pre>

<p>We can easily inspect the traceplots of the hyperparameters (each color stands for a different chain):</p>

<pre><code class="language-python">plt.subplot(121)
plt.plot(a_bars.numpy(),alpha=0.3)
plt.subplot(122)
plt.plot(sigmas.numpy(),alpha=0.3)
</code></pre>

<p><img src="output_34_0.png" alt="png" /></p>

<p>We get nicely mixed chains, which is good. We can also plot the posterior distributions of the logits for the different tanks:</p>

<pre><code class="language-python">plt.figure(figsize=(12,9))
for i in range(df.shape[0]):
    plt.subplot(7,7,i+1)
    for j in range(n_chains):
        sns.kdeplot(np.array(logits[:,j,i]))
plt.tight_layout()
</code></pre>

<p><img src="output_36_0.png" alt="png" /></p>

<p>Here, each subplot corresponds to one tank, and different colors represent different chains. Just by eye-balling the posteriors, we can see a lot of variability between tanks; this is obvious when we compute posterior survival probabilities themselves:</p>

<pre><code class="language-python">ps = tf.sigmoid(logits).numpy()

plt.figure(figsize=(12,9))
for i in range(df.shape[0]):
    current_ps = ps[:,:,i]
    pred = plt.errorbar(x=[i],y=[current_ps.mean()],
                     yerr=np.array([current_ps.mean()-np.quantile(current_ps,0.25),
                                    np.quantile(current_ps,0.75)-current_ps.mean()]).reshape(2,-1),
                     fmt='o',c='k')
    act = plt.scatter(i,df.loc[i,'propsurv'],c='r')
plt.grid()
plt.xlabel(&quot;Tank number&quot;,fontsize=20)
plt.ylabel(&quot;Survival probability&quot;,fontsize=20)
plt.axhline(df.surv.sum()/df.density.sum(),lw=1)
for density_change in np.where(df.density.diff())[0][1:]:
    plt.axvline(density_change,ls='--',c='k',lw=1)
plt.legend([pred,act],['50% Prediction Interval','propsurv'])
</code></pre>

<p><img src="output_38_1.png" alt="png" /></p>

<p>The black dots are the posterior mean probabilities, the errorbars represent the interquartile range, the red dots are <code>propsurv</code> (the no-pooling estimate), and the blue horizontal line is the grand mean (the complete-pooling estimate). Vertical lines split the tanks to densities 10, 25 and 35. We can see that, as expected, posterior probabilities are shrunk towards the grand mean. We can also plot the difference between the posterior means and <code>propsurv</code>, to observe that shrinkage is indeed larger when the sample size is smaller:</p>

<pre><code class="language-python">for i in range(df.shape[0]):
    current_ps = ps[:,:,i]
    plt.scatter(i,(df.loc[i,'propsurv']-current_ps.mean()),c='k')
plt.axhline(0,lw=1)
for density_change in np.where(df.density.diff())[0][1:]:
    plt.axvline(density_change,ls='--',c='k',lw=1)
plt.xlabel(&quot;Tank number&quot;,fontsize=20)
plt.ylabel(&quot;Shrinkage&quot;,fontsize=20)
</code></pre>

<p><img src="output_40_1.png" alt="png" /></p>

<h1 id="wrapping-up">Wrapping up</h1>

<p>TFP certainly has a different feel to it compared to other probabilistic programming frameworks like PyMC3 or Stan; specifically, the introduction of batching semantics, and the complexity of the API that is exposed, are very different and pose a real learning curve. The slope, I guess, depends on one&rsquo;s background.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/tfp/">TFP</a>
  
  <a class="badge badge-light" href="/tags/multilevel-models/">Multilevel Models</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://adamhaber.github.io/post/varying-intercepts/&amp;text=A%20Tutorial%20on%20Varying%20Intercepts%20Models%20with%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://adamhaber.github.io/post/varying-intercepts/&amp;t=A%20Tutorial%20on%20Varying%20Intercepts%20Models%20with%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=A%20Tutorial%20on%20Varying%20Intercepts%20Models%20with%20TensorFlow%20Probability&amp;body=https://adamhaber.github.io/post/varying-intercepts/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://adamhaber.github.io/post/varying-intercepts/&amp;title=A%20Tutorial%20on%20Varying%20Intercepts%20Models%20with%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=A%20Tutorial%20on%20Varying%20Intercepts%20Models%20with%20TensorFlow%20Probability%20https://adamhaber.github.io/post/varying-intercepts/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://adamhaber.github.io/post/varying-intercepts/&amp;title=A%20Tutorial%20on%20Varying%20Intercepts%20Models%20with%20TensorFlow%20Probability" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hud138a96a47a9e2657537fb19ee618cb6_16021_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://adamhaber.github.io/">Adam Haber</a></h5>
      <h6 class="card-subtitle">Computational Neuroscience PhD Student</h6>
      <p class="card-text">Interested in probabilistic programming, computational statistics, statistical physics and programming languages.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#mailto:adamhaber@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/_adam_haber" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/adamhaber" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>






<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>









  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.96cf4c3dc37ea60dbbd03c13a455f1f7.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
